{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e3660f-bc74-41a4-b63a-9adcf33e895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # List of CSV files to merge\n",
    "# csv_files = ['dares/test.csv', 'dares/train.csv', 'dares/validation.csv']\n",
    "\n",
    "# # Read and concatenate all CSV files\n",
    "# merged_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "# # Save the merged dataframe to a new CSV file\n",
    "# merged_df.to_csv('merged_output.csv', index=False)\n",
    "\n",
    "# print(\"CSV files merged successfully into 'merged_output.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93429a19-5f2a-415a-940e-72859da0369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35aff5a-7053-4235-93aa-d3a6f3b42fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merged_output.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9cd9b2b-3b42-4308-9e87-92635f2147ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math = df[df[\"English_Filename\"]==\"Math\"].reset_index(drop=True)\n",
    "df_Science = df[df[\"English_Filename\"]==\"Science\"].reset_index(drop=True)\n",
    "df_Physics = df[df[\"English_Filename\"]==\"Physics\"].reset_index(drop=True)\n",
    "df_Ecology = df[df[\"English_Filename\"]==\"Ecology\"].reset_index(drop=True)\n",
    "df_Chemistry = df[df[\"English_Filename\"]==\"Chemistry\"].reset_index(drop=True)\n",
    "df_Biology = df[df[\"English_Filename\"]==\"Biology\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de16723-25f6-481b-b31b-365d7b580b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Arabic1 = df[df[\"English_Filename\"]==\"ArabicLanguage\"].reset_index(drop=True)\n",
    "df_Arabic2 = df[df[\"English_Filename\"]==\"Arabic_Language\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "278c1ff5-d9ca-4f76-8aba-c313bedacfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Arabic = pd.concat([df_Arabic1,df_Arabic2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1636c511-9148-45af-a259-3547049c542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "# import numpy as np\n",
    "# from FlagEmbedding import BGEM3FlagModel\n",
    "# import torch\n",
    "# from tqdm.notebook import tqdm\n",
    "# import pickle\n",
    "\n",
    "# # Ensure GPU is used if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if device.type != \"cuda\":\n",
    "#     raise RuntimeError(\"CUDA device not available. Ensure GPU and faiss-gpu are properly installed.\")\n",
    "\n",
    "# # Initialize the BGE-M3 model\n",
    "# model = BGEM3FlagModel('BAAI/bge-m3', device=device)\n",
    "\n",
    "# # Initialize FAISS GPU resources and index\n",
    "# dimension = 1024  # BGE-M3 embedding dimension\n",
    "# res = faiss.StandardGpuResources()  # Create GPU resources\n",
    "# index = faiss.IndexFlatL2(dimension)  # Create CPU index\n",
    "# gpu_index = faiss.index_cpu_to_gpu(res, 0, index)  # Move index to GPU (device 0)\n",
    "\n",
    "# # List to store metadata\n",
    "# metadata = []\n",
    "\n",
    "# # Function to process and add embeddings for a given dataframe\n",
    "# def add_embeddings_to_index(df, subject_name, index, metadata_list):\n",
    "#     embeddings = []\n",
    "#     for i in tqdm(range(len(df)), desc=f\"Encoding {subject_name}\"):\n",
    "#         emb = model.encode(df[\"Text\"][i], max_length=256)['dense_vecs']\n",
    "#         embeddings.append(emb)\n",
    "        \n",
    "#         # Store only text and subject as metadata\n",
    "#         meta = {'subject': subject_name, 'text': df[\"Text\"][i]}\n",
    "#         metadata_list.append(meta)\n",
    "    \n",
    "#     # Convert embeddings to numpy array and add to FAISS GPU index\n",
    "#     embeddings = np.array(embeddings).astype('float32')\n",
    "#     index.add(embeddings)\n",
    "#     return embeddings\n",
    "\n",
    "# # Process each subject's dataframe\n",
    "# subjects = [\n",
    "#     ('Math', df_math),\n",
    "#     ('Arabic', df_Arabic),\n",
    "#     ('Science', df_Science),\n",
    "#     ('Physics', df_Physics),\n",
    "#     ('Ecology', df_Ecology),\n",
    "#     ('Chemistry', df_Chemistry),\n",
    "#     ('Biology', df_Biology)\n",
    "# ]\n",
    "\n",
    "# for subject_name, df in subjects:\n",
    "#     add_embeddings_to_index(df, subject_name, gpu_index, metadata)\n",
    "\n",
    "# # Move index back to CPU for saving (optional, if you need CPU compatibility)\n",
    "# cpu_index = faiss.index_gpu_to_cpu(gpu_index)\n",
    "# faiss.write_index(cpu_index, \"subject_embeddings.faiss\")\n",
    "\n",
    "# # Save metadata\n",
    "# with open(\"subject_metadata.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(metadata, f)\n",
    "\n",
    "# print(f\"FAISS GPU index created with {gpu_index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7a6b769-c12e-420a-80bf-d1e6591c780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "# import numpy as np\n",
    "# from FlagEmbedding import BGEM3FlagModel\n",
    "# import torch\n",
    "# import pickle\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Ensure GPU is used if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if device.type != \"cuda\":\n",
    "#     raise RuntimeError(\"CUDA device not available. Ensure GPU and faiss-gpu are properly installed.\")\n",
    "\n",
    "# # Initialize the BGE-M3 model\n",
    "# model = BGEM3FlagModel('BAAI/bge-m3', device=device)\n",
    "\n",
    "# # Load the FAISS index\n",
    "# cpu_index = faiss.read_index(\"subject_embeddings.faiss\")\n",
    "\n",
    "# # Move index to GPU\n",
    "# res = faiss.StandardGpuResources()\n",
    "# gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "\n",
    "# # Load metadata\n",
    "# with open(\"subject_metadata.pkl\", \"rb\") as f:\n",
    "#     metadata = pickle.load(f)\n",
    "\n",
    "# # Function to query the FAISS index with subject filter\n",
    "# def search_faiss_with_subject_filter(query_text, index, model, metadata, subject_filter=None, top_k=5):\n",
    "#     # Encode the query text\n",
    "#     query_embedding = model.encode(query_text, max_length=256)['dense_vecs']\n",
    "#     query_embedding = np.array([query_embedding]).astype('float32')  # Shape: (1, dimension)\n",
    "\n",
    "#     # Perform similarity search (retrieve more results to account for filtering)\n",
    "#     search_k = top_k * 10  # Oversample to ensure enough results after filtering\n",
    "#     distances, indices = index.search(query_embedding, search_k)\n",
    "\n",
    "#     # Filter results by subject and collect top-k\n",
    "#     results = []\n",
    "#     for i, idx in enumerate(indices[0]):\n",
    "#         if idx >= len(metadata):  # Safety check for index bounds\n",
    "#             continue\n",
    "#         meta = metadata[idx]\n",
    "#         # Apply subject filter (case-insensitive)\n",
    "#         if subject_filter is None or meta['subject'].lower() == subject_filter.lower():\n",
    "#             result = {\n",
    "#                 'index': int(idx),\n",
    "#                 'distance': float(distances[0][i]),\n",
    "#                 'subject': meta['subject'],\n",
    "#                 'text': meta['text']\n",
    "#             }\n",
    "#             results.append(result)\n",
    "#         if len(results) >= top_k:  # Stop once we have enough filtered results\n",
    "#             break\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Example query with subject filter\n",
    "# query = \"ما هي الخليه\"\n",
    "# subject_filter = None  # Set to None for no filtering, or specify a subject like \"Math\"\n",
    "# top_k = 5  # Number of top results to retrieve\n",
    "# results = search_faiss_with_subject_filter(query, gpu_index, model, metadata, subject_filter, top_k)\n",
    "\n",
    "# # Print results\n",
    "# print(f\"Query: {query}\")\n",
    "# print(f\"Subject Filter: {subject_filter if subject_filter else 'None'}\")\n",
    "# print(f\"Top {len(results)} results (up to {top_k}):\")\n",
    "# for i, result in enumerate(results, 1):\n",
    "#     print(f\"\\nResult {i}:\")\n",
    "#     print(f\"Subject: {result['subject']}\")\n",
    "#     print(f\"Text: {result['text']}\")\n",
    "#     print(f\"Distance: {result['distance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ef10e-1dc8-43ff-9ab0-8abeca2aeb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5c63c5686e49e28aa0ac48eb33d8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبًا يا أصدقاء! أنا معلم ودود هنا لمساعدتكم في الدراسة من الصف الأول إلى الصف الثاني عشر. اسألوني سؤالًا بالعربية (اكتب 'خروج' للخروج):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "السؤال:  ما هي القوانين في الفيزياء؟\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الإجابة: القوام في الفيزياء هي قوانين تساعد في فهم الظواهر الطبيعية. ومنها:\n",
      "\n",
      "- القوانين الأولية في الفيزياء، مثل قوانين نيوتن، تُوضح أن الحركة تظل كما هي إذا لم تكن القوة المحصلة على الجسم متساوية بالصفر.\n",
      "- القوانين الأخرى مثل القوانين الفيزيائية لنيوتن، والتي توضّحت أن القوى تتأثر بشكل معاكس بينها، كما تساعد في تنبؤات مثل تجربة نيوتن والمحركات.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "السؤال:  ما هي الخليه؟\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الإجابة: الخليه هي الوحدة الأساسية لبناء الجسم الحي، وتكون من أجزاء الجسم. تتكون الخلايا من مواد مختلفة تؤدي وظائف محددة. الخلايا الحية تحتوي على عدة أجزاء داخلية، مثل النواة، السيتوبلازما، وغشاء الخلوي. تساعد الخلايا في تنظيم وظائف الجسم، مثل توزيع الوراثة والتكاثر. كل خلاصة تلعب دورًا مركبًا في الحفاظ على صحة الكائن الحي.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List, Dict\n",
    "import pickle\n",
    "\n",
    "# Ensure GPU is used if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type != \"cuda\":\n",
    "    print(\"Warning: CUDA not available. Using CPU, which may be slower.\")\n",
    "\n",
    "# Initialize the BGE-M3 model for embeddings\n",
    "embedding_model = BGEM3FlagModel('BAAI/bge-m3', device=device)\n",
    "\n",
    "# Load the FAISS index\n",
    "cpu_index = faiss.read_index(\"subject_embeddings.faiss\")\n",
    "\n",
    "# Move index to GPU if available\n",
    "if device.type == \"cuda\":\n",
    "    res = faiss.StandardGpuResources()\n",
    "    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "else:\n",
    "    gpu_index = cpu_index  # Fallback to CPU index\n",
    "\n",
    "# Load metadata\n",
    "with open(\"subject_metadata.pkl\", \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# Initialize the Qwen3-0.6B model and tokenizer\n",
    "llm_model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "# Function to query FAISS without subject filter and no oversampling\n",
    "def search_faiss(query_text: str, index, embedding_model, metadata: List[Dict], top_k: int = 3) -> List[Dict]:\n",
    "    query_embedding = embedding_model.encode(query_text, max_length=256)['dense_vecs']\n",
    "    query_embedding = np.array([query_embedding]).astype('float32')\n",
    "\n",
    "    search_k = top_k\n",
    "    distances, indices = index.search(query_embedding, search_k)\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx >= len(metadata):\n",
    "            continue\n",
    "        result = {\n",
    "            'index': int(idx),\n",
    "            'distance': float(distances[0][i]),\n",
    "            'subject': metadata[idx]['subject'],\n",
    "            'text': metadata[idx]['text']\n",
    "        }\n",
    "        results.append(result)\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to call the Qwen3-0.6B model with chat template\n",
    "def call_llm(prompt: str, history: List[Dict[str, str]] = [], max_new_tokens: int = 200) -> str:\n",
    "    # Prepare chat history\n",
    "    messages = []\n",
    "    for turn in history[-3:]:\n",
    "        messages.append({\"role\": \"user\", \"content\": turn['user']})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": turn['assistant']})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Apply chat template with enable_thinking=False\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(llm_model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generated_ids = llm_model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "    # Decode the response\n",
    "    content = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "    return content\n",
    "\n",
    "# Conversational RAG function\n",
    "def conversational_rag(query: str, history: List[Dict[str, str]]) -> Dict[str, str]:\n",
    "    results = search_faiss(query, gpu_index, embedding_model, metadata, top_k=3)\n",
    "    context = \"\\n\".join([f\"- {result['text']} (موضوع: {result['subject']})\" for result in results])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "أنا معلم ودود أشرح المواد المدرسية للطلاب من الصف الأول إلى الصف الثاني عشر. سأقدم الإجابات باللغة العربية فقط بأسلوب بسيط وودود يناسب مستواكم. لا أستخدم أي لغة أخرى.\n",
    "السياق (استخدمه إذا كان مفيدًا):\n",
    "{context}\n",
    "\n",
    "السؤال: {query}\n",
    "أجب بالعربية الفصحى بطريقة ودودة وسهلة الفهم تناسب الطلاب من جميع المراحل الدراسية، وإذا لم يكن السياق كافيًا، استخدم معرفتي لتوضيح الموضوع بشكل ممتع!\n",
    "\"\"\"\n",
    "    \n",
    "    response = call_llm(prompt, history)\n",
    "    history.append({\"user\": query, \"assistant\": response})\n",
    "    \n",
    "    return {\"content\": response, \"history\": history}\n",
    "\n",
    "# Example conversational loop\n",
    "def main():\n",
    "    history = []\n",
    "    print(\"مرحبًا يا أصدقاء! أنا معلم ودود هنا لمساعدتكم في الدراسة من الصف الأول إلى الصف الثاني عشر. اسألوني سؤالًا بالعربية (اكتب 'خروج' للخروج):\")\n",
    "    while True:\n",
    "        query = input(\"السؤال: \")\n",
    "        if query.strip() == \"خروج\":\n",
    "            break\n",
    "        result = conversational_rag(query, history)\n",
    "        print(f\"الإجابة: {result['content']}\\n\")\n",
    "        history = result['history'][-10:]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
