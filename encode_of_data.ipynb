{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e3660f-bc74-41a4-b63a-9adcf33e895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # List of CSV files to merge\n",
    "# csv_files = ['dares/test.csv', 'dares/train.csv', 'dares/validation.csv']\n",
    "\n",
    "# # Read and concatenate all CSV files\n",
    "# merged_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "# # Save the merged dataframe to a new CSV file\n",
    "# merged_df.to_csv('merged_output.csv', index=False)\n",
    "\n",
    "# print(\"CSV files merged successfully into 'merged_output.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93429a19-5f2a-415a-940e-72859da0369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35aff5a-7053-4235-93aa-d3a6f3b42fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merged_output.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9cd9b2b-3b42-4308-9e87-92635f2147ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math = df[df[\"English_Filename\"]==\"Math\"].reset_index(drop=True)\n",
    "df_Science = df[df[\"English_Filename\"]==\"Science\"].reset_index(drop=True)\n",
    "df_Physics = df[df[\"English_Filename\"]==\"Physics\"].reset_index(drop=True)\n",
    "df_Ecology = df[df[\"English_Filename\"]==\"Ecology\"].reset_index(drop=True)\n",
    "df_Chemistry = df[df[\"English_Filename\"]==\"Chemistry\"].reset_index(drop=True)\n",
    "df_Biology = df[df[\"English_Filename\"]==\"Biology\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de16723-25f6-481b-b31b-365d7b580b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Arabic1 = df[df[\"English_Filename\"]==\"ArabicLanguage\"].reset_index(drop=True)\n",
    "df_Arabic2 = df[df[\"English_Filename\"]==\"Arabic_Language\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "278c1ff5-d9ca-4f76-8aba-c313bedacfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Arabic = pd.concat([df_Arabic1,df_Arabic2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1636c511-9148-45af-a259-3547049c542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "# import numpy as np\n",
    "# from FlagEmbedding import BGEM3FlagModel\n",
    "# import torch\n",
    "# from tqdm.notebook import tqdm\n",
    "# import pickle\n",
    "\n",
    "# # Ensure GPU is used if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if device.type != \"cuda\":\n",
    "#     raise RuntimeError(\"CUDA device not available. Ensure GPU and faiss-gpu are properly installed.\")\n",
    "\n",
    "# # Initialize the BGE-M3 model\n",
    "# model = BGEM3FlagModel('BAAI/bge-m3', device=device)\n",
    "\n",
    "# # Initialize FAISS GPU resources and index\n",
    "# dimension = 1024  # BGE-M3 embedding dimension\n",
    "# res = faiss.StandardGpuResources()  # Create GPU resources\n",
    "# index = faiss.IndexFlatL2(dimension)  # Create CPU index\n",
    "# gpu_index = faiss.index_cpu_to_gpu(res, 0, index)  # Move index to GPU (device 0)\n",
    "\n",
    "# # List to store metadata\n",
    "# metadata = []\n",
    "\n",
    "# # Function to process and add embeddings for a given dataframe\n",
    "# def add_embeddings_to_index(df, subject_name, index, metadata_list):\n",
    "#     embeddings = []\n",
    "#     for i in tqdm(range(len(df)), desc=f\"Encoding {subject_name}\"):\n",
    "#         emb = model.encode(df[\"Text\"][i], max_length=256)['dense_vecs']\n",
    "#         embeddings.append(emb)\n",
    "        \n",
    "#         # Store only text and subject as metadata\n",
    "#         meta = {'subject': subject_name, 'text': df[\"Text\"][i]}\n",
    "#         metadata_list.append(meta)\n",
    "    \n",
    "#     # Convert embeddings to numpy array and add to FAISS GPU index\n",
    "#     embeddings = np.array(embeddings).astype('float32')\n",
    "#     index.add(embeddings)\n",
    "#     return embeddings\n",
    "\n",
    "# # Process each subject's dataframe\n",
    "# subjects = [\n",
    "#     ('Math', df_math),\n",
    "#     ('Arabic', df_Arabic),\n",
    "#     ('Science', df_Science),\n",
    "#     ('Physics', df_Physics),\n",
    "#     ('Ecology', df_Ecology),\n",
    "#     ('Chemistry', df_Chemistry),\n",
    "#     ('Biology', df_Biology)\n",
    "# ]\n",
    "\n",
    "# for subject_name, df in subjects:\n",
    "#     add_embeddings_to_index(df, subject_name, gpu_index, metadata)\n",
    "\n",
    "# # Move index back to CPU for saving (optional, if you need CPU compatibility)\n",
    "# cpu_index = faiss.index_gpu_to_cpu(gpu_index)\n",
    "# faiss.write_index(cpu_index, \"subject_embeddings.faiss\")\n",
    "\n",
    "# # Save metadata\n",
    "# with open(\"subject_metadata.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(metadata, f)\n",
    "\n",
    "# print(f\"FAISS GPU index created with {gpu_index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a6b769-c12e-420a-80bf-d1e6591c780e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9f471a4888424eb8bd48db01dd7c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ŸÖÿß ŸáŸà ÿßŸÑŸÅÿπŸÑ\n",
      "Subject Filter: None\n",
      "Top 5 results (up to 5):\n",
      "\n",
      "Result 1:\n",
      "Subject: Arabic\n",
      "Text: ÿßŸÑÿ¨ŸÖŸÑÿ© ÿßŸÑŸÅÿπŸÑŸäÿ© ÿ™ÿ™ÿ£ŸÑŸÅ ŸÖŸÜ ÿ±ŸÉŸÜŸäŸÜ ÿ£ÿ≥ÿßÿ≥ŸäŸäŸÜ ÿßŸÑŸÅÿπŸÑ ŸàÿßŸÑŸÅÿßÿπŸÑ. ÿßŸÑŸÅÿßÿπŸÑ ŸáŸà ÿßÿ≥ŸÖ ŸÖÿ±ŸÅŸàÿπ ŸäÿØŸÑ ÿπŸÑŸâ ÿßŸÑÿ¥ÿÆÿµ ÿßŸÑŸÇÿßÿ¶ŸÖ ÿ®ÿßŸÑŸÅÿπŸÑ. ÿπŸÑÿßŸÖÿ© ÿ±ŸÅÿπ ÿßŸÑŸÅÿßÿπŸÑ ŸáŸä ÿßŸÑÿ∂ŸÖÿ© ÿßŸÑÿ™Ÿä ÿ™ÿ∏Ÿáÿ± ÿπŸÑŸâ ÿßŸÑÿ≠ÿ±ŸÅ ÿßŸÑÿ£ÿÆŸäÿ±. ÿ£ÿ∂ÿπ ÿßŸÑÿ£ÿ≥ŸÖÿßÿ° ÿßŸÑÿ™ÿßŸÑŸäÿ© ŸÅŸä ÿ¨ŸÖŸÑÿ© ÿ™ŸÉŸàŸÜ ŸÅŸäŸáÿß ŸÅÿßÿπŸÑÿßÿå ÿ´ŸÖ ÿ£ÿ±ÿØÿØŸáÿß ÿ®ŸÜÿ∑ŸÇ ÿ≥ŸÑŸäŸÖ. ÿ£ÿ∂ÿπ ŸÅÿßÿπŸÑÿß ŸÑŸÉŸÑ ŸÅÿπŸÑ ŸÖŸÜ ÿßŸÑÿ£ŸÅÿπÿßŸÑ ÿßŸÑÿ™ÿßŸÑŸäÿ© ÿ´ŸÖ ÿ£ÿ±ÿØÿØŸáÿß ÿ®ÿ™ŸÑÿßŸàÿ© ÿ≥ŸÑŸäŸÖÿ©. ÿßŸÇÿ±ÿ£ ÿßŸÑŸÅŸÇÿ±ÿ© ÿßŸÑÿ™ÿßŸÑŸäÿ©ÿå Ÿàÿπÿ±ŸÅ ÿßŸÑŸÅÿπŸÑ ŸàÿßŸÑŸÅÿßÿπŸÑÿå ŸàŸÜÿ∑ŸÇŸáŸÖÿß ŸÜÿ∑ŸÇÿß ÿ≥ŸÑŸäŸÖÿß ÿ¥ÿ±ŸÇÿ™ ÿ¥ŸÖÿ≥ ÿßŸÑÿπŸäÿØ ŸÅÿ®ÿßÿØÿ±ÿ™ ÿßŸÑÿ≥ÿπÿßÿØÿ© ÿ®ÿßŸÑÿ¨ŸÖŸäÿπÿå Ÿàÿ∏Ÿáÿ±ÿ™ ŸÖÿ∏ÿßŸáÿ± ÿßŸÑÿπŸäÿØ ÿπŸÑŸâ ŸÉŸÑ ÿßŸÑŸàÿ¨ŸàŸáÿå ŸàŸÅŸä ŸÉŸÑ ÿßŸÑÿ®ŸäŸàÿ™ ŸàÿßŸÑÿ¥Ÿàÿßÿ±ÿπ ÿ™ÿ®ÿ™ÿ≥ŸÖ ÿßŸÑÿ£ŸÅŸàÿßŸá Ÿàÿ™ÿ±ÿØÿØ ÿßŸÑÿ£ŸÑÿ≥ŸÜ ÿßŸÑÿπÿ®ÿßÿ±ÿßÿ™ ÿßŸÑÿ™ŸáŸÜÿ¶ÿ©. Ÿäÿ±ÿ™ÿØŸä ÿßŸÑÿµÿ∫Ÿäÿ± ŸàÿßŸÑŸÉÿ®Ÿäÿ± ÿßŸÑŸÖŸÑÿßÿ®ÿ≥ ÿßŸÑÿ¨ÿØŸäÿØÿ©ÿå ŸàŸäÿ≠ÿµŸÑ ÿßŸÑÿ£ÿ∑ŸÅÿßŸÑ ÿπŸÑŸâ ÿßŸÑÿπŸäÿØŸäÿßÿ™. ÿ£ÿ¨ÿ® ÿπŸÜ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿ™ÿßŸÑŸäÿ© ÿ®ÿ¨ŸÖŸÑÿ© ŸÅÿπŸÑŸäÿ© ŸàÿßŸÇÿ±ÿ£Ÿáÿß ÿ®ÿßŸÑÿ∂ÿ®ÿ∑ ÿßŸÑÿµÿ≠Ÿäÿ≠ ŸÖÿßÿ∞ÿß ŸÅÿπŸÑ ÿßŸÑÿ™ŸÑŸÖŸäÿ∞ ŸÅŸä ÿßŸÑÿØÿ±ÿ≥ÿü ÿ£ÿ¨ÿπŸÑ ÿßŸÑŸÅÿßÿπŸÑ ŸÖŸÅÿ±ÿØÿß ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑ ÿßŸÑÿ™ÿßŸÑŸäÿ© Ÿàÿ∂ÿ®ÿ∑Ÿáÿß ÿ®ÿßŸÑÿ¥ŸÉŸÑ ÿ∑ÿßŸÅ ÿßŸÑÿ≠ÿßÿ¨ ÿ≠ŸàŸÑ ÿßŸÑŸÉÿπÿ®ÿ©. ÿ£ÿ¨ÿπŸÑ ÿßŸÑŸÅÿßÿπŸÑ ÿ¨ŸÖÿπÿß ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑ ÿßŸÑÿ™ÿßŸÑŸäÿ© Ÿàÿ∂ÿ®ÿ∑Ÿáÿß ÿ®ÿßŸÑÿ¥ŸÉŸÑ ÿßŸÜÿ™ÿµÿ± ÿßŸÑÿ¨ŸÜŸàÿØ ÿπŸÑŸâ ÿßŸÑÿ£ÿπÿØÿßÿ°. ŸäŸÑÿπÿ® ÿßŸÑÿ£ŸàŸÑÿßÿØ ÿ®ÿßŸÑŸÉÿ±ÿ©.\n",
      "Distance: 0.7882\n",
      "\n",
      "Result 2:\n",
      "Subject: Arabic\n",
      "Text: Ÿàÿßÿ≥ÿ™ÿ®ÿØŸÑ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ∫Ÿäÿ± ÿßŸÑŸÖÿ≥ÿ™ÿπŸÖŸÑÿ© ÿ£Ÿà ÿ∫Ÿäÿ± ÿßŸÑŸÖŸÅŸáŸàŸÖÿ© ÿ£Ÿà ÿßŸÑŸÖŸÉÿ±ÿ±ÿ© ÿ®ŸÖÿß ŸäŸÜÿßÿ≥ÿ® ÿßŸÑÿ≥ŸäÿßŸÇÿå Ÿàÿßÿ≠ÿ∞ŸÅ ŸÉŸÑŸÖÿ© ÿßŸÑÿ¥ŸÉŸÑ ŸàŸÉŸÑŸÖÿ© ÿßŸÑÿ±ŸÇŸÖ. ŸàÿßŸÑŸÅÿßÿπŸÑ ŸáŸà ÿßŸÑÿßÿ≥ŸÖ ÿßŸÑŸÖÿ±ŸÅŸàÿπ ÿßŸÑÿ∞Ÿä ŸäŸÑŸä ÿßŸÑŸÅÿπŸÑ ŸÑŸÑÿØŸÑÿßŸÑÿ© ÿπŸÑŸâ ŸÖŸÜ ŸÜŸÅÿ∞ ÿßŸÑŸÅÿπŸÑ. ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© Ÿäÿ¨ŸÑÿ≥ ÿÆÿßŸÑÿØÿå ŸÉŸÑŸÖÿ© ÿÆÿßŸÑÿØ ŸáŸä ÿßÿ≥ŸÖ ŸÖÿ±ŸÅŸàÿπ Ÿàÿ™ÿ£ÿ™Ÿä ÿ®ÿπÿØ ÿßŸÑŸÅÿπŸÑ Ÿäÿ¨ŸÑÿ≥ ŸÑŸÑÿØŸÑÿßŸÑÿ© ÿπŸÑŸâ ÿßŸÑÿ¥ÿÆÿµ ÿßŸÑÿ∞Ÿä ŸÇÿßŸÖ ÿ®ÿßŸÑÿ¨ŸÑŸàÿ≥. Ÿàÿ•ÿ∞ÿß ŸÑŸÖ Ÿäÿπÿ±ŸÅ ÿßŸÑŸÅÿßÿπŸÑ ÿ£Ÿà ŸÑŸÖ Ÿäÿ±ÿØ ÿ∞ŸÉÿ±Ÿáÿå Ÿäÿ®ŸÜŸâ ÿßŸÑŸÅÿπŸÑ ÿØŸàŸÜ ÿ™ÿ≠ÿØŸäÿØ ŸÅÿßÿπŸÑŸá ÿ®ÿ™ÿ∫ŸäŸäÿ± ÿµŸäÿßÿ∫ÿ™Ÿáÿå ÿ≠Ÿäÿ´ Ÿäÿ∂ÿßŸÅ ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÅÿπŸàŸÑ ÿ£ŸàŸÑÿß ŸÅŸä ÿßŸÑŸÖÿßÿ∂Ÿä ŸàŸäÿ≠ÿ∞ŸÅ ŸÖÿß ŸÇÿ®ŸÑ ÿ¢ÿÆÿ±Ÿáÿå ŸÖÿ´ŸÑ ŸÇŸàŸÑŸÜÿß ÿ∏ŸÑŸÖÿ™ Ÿàÿ∏ŸÑŸÖ. ŸàŸäÿ∂ÿßŸÅ ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÅÿπŸàŸÑ ÿ£ŸàŸÑÿß ŸÅŸä ÿßŸÑŸÖÿ∂ÿßÿ±ÿπ ŸàŸäŸÅÿ™ÿ≠ ŸÖÿß ŸÇÿ®ŸÑ ÿ¢ÿÆÿ±Ÿáÿå ŸÖÿ´ŸÑ ŸÇŸàŸÑŸÜÿß Ÿäÿ∏ŸÑŸÖ ŸàŸäÿ∏ŸÑŸÖ. ŸàŸáŸÜÿß Ÿäÿµÿ®ÿ≠ ŸÑŸÑŸÅÿπŸÑ ŸÜÿßÿ¶ÿ® ŸÅÿßÿπŸÑ ŸÖÿ±ŸÅŸàÿπÿå ŸÖÿ´ŸÑ ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© ÿ∏ŸÑŸÖ ÿßŸÑÿ≥ÿ¨ŸäŸÜÿå ÿ≠Ÿäÿ´ ŸäÿØŸÑ ÿπŸÑŸâ ÿßŸÑÿ¥ÿÆÿµ ÿßŸÑŸÇÿßŸÖ ÿ®ÿπŸÖŸÑŸäÿ© ÿßŸÑÿ∏ŸÑŸÖ.\n",
      "Distance: 0.8973\n",
      "\n",
      "Result 3:\n",
      "Subject: Arabic\n",
      "Text: ÿµÿ≠Ÿäÿ≠ ÿßŸÑÿ•ŸÖŸÑÿßÿ° ŸàÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÅŸä ÿßŸÑŸÜÿµŸàÿµÿå Ÿàÿßÿ≥ÿ™ÿ®ÿØŸÑ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ∫Ÿäÿ± ÿßŸÑŸÖŸÜÿßÿ≥ÿ®ÿ© ÿ®ŸÖÿß ŸäŸÜÿßÿ≥ÿ® ÿßŸÑÿ≥ŸäÿßŸÇ. ÿßŸÑŸÅÿßÿπŸÑ ŸáŸà ÿßŸÑÿ∞Ÿä ŸäŸÇŸàŸÖ ÿ®ÿ£ÿØÿßÿ° ÿßŸÑŸÅÿπŸÑÿå ŸàŸáŸà ÿ±ŸÉŸÜ ÿ£ÿ≥ÿßÿ≥Ÿä ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ©. ÿßŸÑÿ¥ŸÉÿ± ŸÑŸÑŸá ÿØÿßÿ¶ŸÖÿß ŸÖŸÜ ÿ≥ŸÖÿßÿ™ ÿßŸÑŸÖÿ§ŸÖŸÜŸäŸÜ. ÿßŸÑÿ∑ÿßŸÑÿ®ÿ© ÿ¨ÿßÿØÿ© ŸÅŸä ÿØÿ±ÿßÿ≥ÿ™Ÿáÿß. ÿßŸÑÿ¨ŸÖŸÑÿ© ÿßŸÑŸÅÿπŸÑŸäÿ© ÿ™ÿ™ŸÉŸàŸÜ ŸÖŸÜ ÿßŸÑŸÅÿßÿπŸÑ ŸàÿßŸÑŸÖŸÅÿπŸàŸÑÿå ŸàŸáŸÖÿß ÿ±ŸÉŸÜÿßŸÜ ÿ£ÿ≥ÿßÿ≥ŸäÿßŸÜ. ÿßŸÑÿÆÿ®ÿ± ŸàÿßŸÑŸÅÿßÿπŸÑ ŸäŸÉŸàŸÜÿßŸÜ ŸÅŸä ÿ≠ÿßŸÑÿ© ÿßŸÑÿ±ŸÅÿπ ÿØÿßÿ¶ŸÖÿß. ÿßŸÑÿ£ÿ≥ŸÖÿßÿ° ŸÇÿØ ÿ™ŸÉŸàŸÜ ŸÅŸä ÿ≠ÿßŸÑÿ© ÿßŸÑÿ±ŸÅÿπ ÿ£Ÿà ÿßŸÑŸÜÿµÿ® ÿ£Ÿà ÿßŸÑÿ¨ÿ±. ÿßŸÑÿ¨ŸÖŸÑÿ© ÿßŸÑÿßÿ≥ŸÖŸäÿ© ÿ™ÿ≠ŸàŸä ÿßŸÑÿ±ŸÉŸÜ ÿßŸÑÿ´ÿßŸÜŸä ÿßŸÑÿ∂ÿ±Ÿàÿ±Ÿä ŸÑÿ™ŸÉÿ™ŸÖŸÑ ŸÖÿπÿßŸÜŸäŸáÿß. ÿßŸÑŸáŸÖÿ≤ÿ© ŸÑÿß ÿ™ŸÜÿ∑ŸÇ ŸÅŸä ÿ®ÿØÿßŸäÿ© ÿßŸÑŸÉŸÑÿßŸÖ. ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑŸÖÿ™ÿπÿØÿØÿ© ŸÇÿØ ÿ™ŸÉŸàŸÜ ŸÑŸáÿß ŸÖŸÅŸáŸàŸÖ Ÿàÿßÿ≠ÿØ. ŸÖŸÜ ÿßŸÑŸÖÿ¥ÿ™ŸÇÿßÿ™ ÿØÿßŸÑÿ© ÿßŸÑŸÖŸÅÿπŸàŸÑÿå ŸàŸáŸä ÿ™ÿ£ÿ™Ÿä ŸÖŸÜ ÿßŸÑŸÅÿπŸÑ ÿßŸÑÿ£ÿµŸÑŸä. ÿØÿπÿßÿ° ÿßŸÑŸÖÿ∏ŸÑŸàŸÖ ÿ∫ÿßŸÑÿ®ÿß ŸÖÿß Ÿäÿ≥ÿ™ÿ¨ÿßÿ®.\n",
      "Distance: 0.9415\n",
      "\n",
      "Result 4:\n",
      "Subject: Arabic\n",
      "Text: ÿ£ŸÜÿß ÿ∑ŸÅŸÑ Ÿäÿ≠ÿ® ÿßŸÑŸÑÿπÿ® ÿπŸÑŸâ ÿ£ÿ∑ÿ±ÿßŸÅ ÿ£ÿµÿßÿ®ÿπŸáÿå ŸàŸàÿßŸÑÿØŸä ÿ®ŸÜÿßÿ° ÿ¥ÿßŸÖÿÆ ŸÑŸÑÿ£ÿ±ŸÉÿßŸÜ. ÿßŸÑŸÅÿπŸÑ ÿßŸÑŸÖÿ∂ÿßÿ±ÿπ Ÿäÿ™ÿ±ÿßŸÅŸÇ ŸÖÿπ ŸÅÿπŸÑ ŸÖÿπÿ±ÿ®. ÿßŸÑŸÅÿπŸÑÿßŸÜ ÿßŸÑŸÖÿßÿ∂Ÿä ŸàÿßŸÑÿ£ŸÖÿ± Ÿäÿ™ŸÖŸäÿ≤ÿßŸÜ ÿ®Ÿàÿ¨ŸàÿØ ÿ•ÿ≠ÿØŸâ ÿßŸÑÿ≠ÿ±ŸàŸÅ. Ÿäÿ™ÿπÿßŸÖŸÑ ÿßŸÑŸÖÿ¨ÿ™ŸÖÿπ ŸÖÿπ ÿßŸÑŸÖÿ≥ŸÜŸäŸÜ ÿ®ŸÉŸÑ ÿ™Ÿàÿßÿ∂ÿπ Ÿàÿßÿ≠ÿ™ÿ±ÿßŸÖ ŸÑŸÜÿ¨ÿπŸÑŸáŸÖ ŸäŸÉŸàŸÜŸàŸÜ ÿ•ÿ∂ÿßŸÅÿ© ÿ•ŸÑŸâ ÿßŸÑŸÖÿ¨ŸÖŸàÿπ. ÿßŸÑŸÅÿπŸÑ ÿßŸÑŸÖÿ∂ÿßÿ±ÿπ ŸäÿµŸÜŸÅ ÿ≠ÿ≥ÿ® ŸÜŸáÿßŸäÿ™Ÿá Ÿàÿ≠ÿßŸÑÿ™Ÿá ÿßŸÑÿ•ÿπÿ±ÿßÿ®Ÿäÿ©ÿå ŸàÿπŸÑÿßŸÖÿ© ÿ•ÿπÿ±ÿßÿ®Ÿá ÿ≠ÿ±ŸÅ ÿßŸÑÿ≥ŸäŸÜ ÿ£Ÿà ÿ≥ŸàŸÅÿå ŸàŸäŸÖŸÉŸÜ ÿ£ŸÜ ÿ™ÿ∂ÿßŸÅ ÿ•ŸÑŸäŸá ÿ≠ÿ±ŸàŸÅ ÿßŸÑŸÜÿµÿ®ÿå ŸÉŸÖÿß ŸÅŸä ÿ≥ÿ£ÿÆÿ±ÿ¨ÿå ÿ≥ŸàŸÅ ÿ£ÿÆÿ±ÿ¨.\n",
      "Distance: 0.9419\n",
      "\n",
      "Result 5:\n",
      "Subject: Arabic\n",
      "Text: ÿ•ŸÜ ÿßŸÑŸàÿ∑ŸÜ ÿπÿ≤Ÿäÿ≤ ÿπŸÑŸâ ÿ£ŸÅÿ±ÿßÿØŸáÿå ŸàŸáÿ∞ÿß ÿ≠ŸÇŸäŸÇÿ© ŸÑÿß ŸäŸÖŸÉŸÜ ÿ•ŸÜŸÉÿßÿ±Ÿáÿß. ÿßŸÑŸÅÿπŸÑ ÿßŸÑŸÖÿ∂ÿßÿ±ÿπ ÿ•ÿ∞ÿß ÿßŸÜÿ™ŸáŸâ ÿ®ÿ¢ÿÆÿ±Ÿá ÿ£ŸÑŸÅ ÿßÿ´ŸÜÿßŸÜ ÿ£Ÿà ŸàÿßŸà ÿßŸÑÿ¨ŸÖÿßÿπÿ© ŸÑŸá ÿµŸàÿ±ÿ™ÿßŸÜ ÿ•ŸÖÿß Ÿäÿ®ÿØÿ£ ÿ®Ÿäÿßÿ° ÿßŸÑÿ∫ÿßÿ¶ÿ® ÿ£Ÿà ÿ™ÿßÿ° ÿ™ŸÅÿπŸÑÿßŸÜ. ÿ£ŸÖÿß ÿßŸÑŸÅÿπŸÑ ÿßŸÑŸÖÿ™ÿµŸÑ ÿ®ŸàÿßŸà ÿßŸÑÿ¨ŸÖÿßÿπÿ© ŸÅŸäŸÉŸàŸÜ ŸÑŸá ÿµŸàÿ±ÿ™ÿßŸÜ ÿ•ŸÖÿß Ÿäÿ®ÿØÿ£ ÿ®ÿßŸÑŸäÿßÿ° ÿ£Ÿà ÿßŸÑÿ™ÿßÿ° ŸÉŸÖÿß ŸÅŸä ÿ≠ÿßŸÑÿ© ÿßŸÑŸÖÿÆÿßÿ∑ÿ®ÿ©. ŸÑŸÖ ŸäŸÉŸÜ ŸäŸÖŸÉŸÜ ÿ£ŸÜ Ÿäÿ®ÿØÿ£ ÿ•ŸÑÿß ÿ®ÿ™ÿßÿ°. ÿßŸÑÿ∞ŸäŸÜ Ÿäÿ≠ÿ®ŸàŸÜ Ÿàÿ∑ŸÜŸáŸÖ Ÿäÿ≠ÿ®ŸàŸÜŸá ÿ®ÿµÿØŸÇ Ÿàÿ•ÿÆŸÑÿßÿµ.\n",
      "Distance: 0.9542\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure GPU is used if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type != \"cuda\":\n",
    "    raise RuntimeError(\"CUDA device not available. Ensure GPU and faiss-gpu are properly installed.\")\n",
    "\n",
    "# Initialize the BGE-M3 model\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', device=device)\n",
    "\n",
    "# Load the FAISS index\n",
    "cpu_index = faiss.read_index(\"subject_embeddings.faiss\")\n",
    "\n",
    "# Move index to GPU\n",
    "res = faiss.StandardGpuResources()\n",
    "gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "\n",
    "# Load metadata\n",
    "with open(\"subject_metadata.pkl\", \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# Function to query the FAISS index with subject filter\n",
    "def search_faiss_with_subject_filter(query_text, index, model, metadata, subject_filter=None, top_k=5):\n",
    "    # Encode the query text\n",
    "    query_embedding = model.encode(query_text, max_length=256)['dense_vecs']\n",
    "    query_embedding = np.array([query_embedding]).astype('float32')  # Shape: (1, dimension)\n",
    "\n",
    "    # Perform similarity search (retrieve more results to account for filtering)\n",
    "    search_k = top_k * 10  # Oversample to ensure enough results after filtering\n",
    "    distances, indices = index.search(query_embedding, search_k)\n",
    "\n",
    "    # Filter results by subject and collect top-k\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx >= len(metadata):  # Safety check for index bounds\n",
    "            continue\n",
    "        meta = metadata[idx]\n",
    "        # Apply subject filter (case-insensitive)\n",
    "        if subject_filter is None or meta['subject'].lower() == subject_filter.lower():\n",
    "            result = {\n",
    "                'index': int(idx),\n",
    "                'distance': float(distances[0][i]),\n",
    "                'subject': meta['subject'],\n",
    "                'text': meta['text']\n",
    "            }\n",
    "            results.append(result)\n",
    "        if len(results) >= top_k:  # Stop once we have enough filtered results\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example query with subject filter\n",
    "query = \"ŸÖÿß ŸáŸà ÿßŸÑŸÅÿπŸÑ\"\n",
    "subject_filter = None  # Set to None for no filtering, or specify a subject like \"Math\"\n",
    "top_k = 5  # Number of top results to retrieve\n",
    "results = search_faiss_with_subject_filter(query, gpu_index, model, metadata, subject_filter, top_k)\n",
    "\n",
    "# Print results\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Subject Filter: {subject_filter if subject_filter else 'None'}\")\n",
    "print(f\"Top {len(results)} results (up to {top_k}):\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Subject: {result['subject']}\")\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Distance: {result['distance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ef10e-1dc8-43ff-9ab0-8abeca2aeb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.6.12: Fast Qwen2 patching. Transformers: 4.53.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.625 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "ŸÖÿ±ÿ≠ÿ®Ÿãÿß Ÿäÿß ÿ£ÿµÿØŸÇÿßÿ°! ÿ£ŸÜÿß ŸÖÿπŸÑŸÖ ŸàÿØŸàÿØ ŸáŸÜÿß ŸÑŸÖÿ≥ÿßÿπÿØÿ™ŸÉŸÖ ŸÅŸä ÿßŸÑÿØÿ±ÿßÿ≥ÿ© ŸÖŸÜ ÿßŸÑÿµŸÅ ÿßŸÑÿ£ŸàŸÑ ÿ•ŸÑŸâ ÿßŸÑÿµŸÅ ÿßŸÑÿ´ÿßŸÜŸä ÿπÿ¥ÿ±. ÿßÿ≥ÿ£ŸÑŸàŸÜŸä ÿ≥ÿ§ÿßŸÑŸãÿß ÿ®ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (ÿßŸÉÿ™ÿ® 'ÿÆÿ±Ÿàÿ¨' ŸÑŸÑÿÆÿ±Ÿàÿ¨):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ÿßŸÑÿ≥ÿ§ÿßŸÑ:  ŸÖÿß ŸáŸà ÿßŸÑŸÅÿπŸÑ ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑŸá ÿßŸÑŸÅÿπŸÑŸäŸá\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©: ÿ®ÿßŸÑÿ∑ÿ®ÿπÿå ÿ≥ÿ£ÿ≥ÿßÿπÿØŸÉ ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑÿ≥ÿ§ÿßŸÑ. ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© \"ŸÇÿßŸÑÿ™ ÿ≤Ÿáÿ±ÿ© ŸÉŸÑŸÖÿ© ÿ≠ŸÑŸàÿ©\" ÿßŸÑŸÅÿπŸÑ ŸáŸÜÿß ŸáŸà \"ŸÇÿßŸÑÿ™\" ŸàŸáŸà ŸÅÿπŸÑ ŸÖÿ∂ÿßÿ±ÿπ ŸÖÿ®ŸÜŸä ÿπŸÑŸâ ÿßŸÑŸÅÿ™ÿ≠ÿå ŸàÿßŸÑŸÅÿßÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿ≤Ÿáÿ±ÿ©\". \n",
      "\n",
      "ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© \"ÿ£ÿπÿ∑Ÿâ ÿ£ÿ≠ŸÖÿØ ŸÉÿ™ÿßÿ®Ÿá ŸÑÿ£ŸÖŸáÿß\" ÿßŸÑŸÅÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿ£ÿπÿ∑Ÿâ\" ŸàŸáŸà ŸÅÿπŸÑ ŸÖÿßÿ∂Ÿç ŸÖÿ®ŸÜŸä ÿπŸÑŸâ ÿßŸÑŸÅÿ™ÿ≠ÿå ŸàÿßŸÑŸÅÿßÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿ£ÿ≠ŸÖÿØ\".\n",
      "\n",
      "ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© \"ÿ™ÿ∞ÿßÿÆÿ± ÿßŸÑÿ£ŸÖÿßŸÜÿßÿ™ ŸÅŸä ÿßŸÑŸÇŸÑŸàÿ®\" ÿßŸÑŸÅÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿ™ÿ∞ÿßÿÆÿ±\" ŸàŸáŸà ŸÅÿπŸÑ ŸÖÿßÿ∂Ÿç ŸÖÿ®ŸÜŸä ÿπŸÑŸâ ÿßŸÑŸÅÿ™ÿ≠ÿå ŸàÿßŸÑŸÅÿßÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿßŸÑÿ£ŸÖÿßŸÜÿßÿ™\".\n",
      "\n",
      "ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© \"ÿ™ÿ≥ŸÑŸÑÿ™ ÿßŸÑŸÜÿ¨ŸàŸÖ ÿ•ŸÑŸâ ÿßŸÑÿ≥ŸÖÿßÿ°\" ÿßŸÑŸÅÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿ™ÿ≥ŸÑŸÑÿ™\" ŸàŸáŸà ŸÅÿπŸÑ ŸÖÿßÿ∂Ÿç ŸÖÿ®ŸÜŸä ÿπŸÑŸâ ÿßŸÑŸÅÿ™ÿ≠ÿå ŸàÿßŸÑŸÅÿßÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿßŸÑŸÜÿ¨ŸàŸÖ\".\n",
      "\n",
      "ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© \"ÿ£ŸÇÿ®ŸÑÿ™ ÿßŸÑÿ¥ŸÖÿ≥ ÿπŸÑŸâ ÿßŸÑÿ£ÿ±ÿ∂\" ÿßŸÑŸÅÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿ£ŸÇÿ®ŸÑÿ™\" ŸàŸáŸà ŸÅÿπŸÑ ŸÖÿßÿ∂Ÿç ŸÖÿ®ŸÜŸä ÿπŸÑŸâ ÿßŸÑŸÅÿ™ÿ≠ÿå ŸàÿßŸÑŸÅÿßÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿßŸÑÿ¥ŸÖÿ≥\".\n",
      "\n",
      "ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© \"ÿ™ÿ¨ŸÖÿπÿ™ ÿßŸÑÿ≠ÿ¥ÿ±ÿßÿ™ ÿ≠ŸàŸÑ ÿßŸÑÿ∑ÿπÿßŸÖ\" ÿßŸÑŸÅÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿ™ÿ¨ŸÖÿπÿ™\" ŸàŸáŸà ŸÅÿπŸÑ ŸÖÿßÿ∂Ÿç ŸÖÿ®ŸÜŸä ÿπŸÑŸâ ÿßŸÑŸÅÿ™ÿ≠ÿå ŸàÿßŸÑŸÅÿßÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿßŸÑÿ≠ÿ¥ÿ±ÿßÿ™\".\n",
      "\n",
      "ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© \"ÿ™ÿπÿØÿØÿ™ ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ ŸÅŸä ÿßŸÑÿ≠ÿ≥ÿßÿ®\" ÿßŸÑŸÅÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿ™ÿπÿØÿØÿ™\" ŸàŸáŸà ŸÅÿπŸÑ ŸÖÿßÿ∂Ÿç ŸÖÿ®ŸÜŸä ÿπŸÑŸâ ÿßŸÑŸÅÿ™ÿ≠ÿå ŸàÿßŸÑŸÅÿßÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿßŸÑÿ£ÿ±ŸÇÿßŸÖ\".\n",
      "\n",
      "ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© \"ÿ™ÿ®ŸÑŸàÿ±ÿ™ ÿßŸÑÿ±ÿ∫ÿ®ÿßÿ™ ŸÅŸä ÿßŸÑÿ£ÿ≠ŸÑÿßŸÖ\" ÿßŸÑŸÅÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿ™ÿ®ŸÑŸàÿ±ÿ™\" ŸàŸáŸà ŸÅÿπŸÑ ŸÖÿßÿ∂Ÿç ŸÖÿ®ŸÜŸä ÿπŸÑŸâ ÿßŸÑŸÅÿ™ÿ≠ÿå ŸàÿßŸÑŸÅÿßÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿßŸÑÿ±ÿ∫ÿ®ÿßÿ™\".\n",
      "\n",
      "ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© \"ÿ™ŸÉÿ™ÿ≥ÿ®ÿ™ ÿßŸÑÿ´ŸÇÿ© ŸÅŸä ÿßŸÑŸÜŸÅÿ≥\" ÿßŸÑŸÅÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿ™ŸÉÿ™ÿ≥ÿ®ÿ™\" ŸàŸáŸà ŸÅÿπŸÑ ŸÖÿßÿ∂Ÿç ŸÖÿ®ŸÜŸä ÿπŸÑŸâ ÿßŸÑŸÅÿ™ÿ≠ÿå ŸàÿßŸÑŸÅÿßÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿßŸÑÿ´ŸÇÿ©\".\n",
      "\n",
      "ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑÿ© \"ÿ™ÿ™ŸàÿßŸÅÿØÿ™ ÿßŸÑÿ£ÿµÿØŸÇÿßÿ° ÿ•ŸÑŸâ ÿßŸÑŸÖŸÜÿ≤ŸÑ\" ÿßŸÑŸÅÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿ™ÿ™ŸàÿßŸÅÿØÿ™\" ŸàŸáŸà ŸÅÿπŸÑ ŸÖÿßÿ∂Ÿç ŸÖÿ®ŸÜŸä ÿπŸÑŸâ ÿßŸÑŸÅÿ™ÿ≠ÿå ŸàÿßŸÑŸÅÿßÿπŸÑ ŸáŸÜÿß ŸáŸà \"ÿßŸÑÿ£ÿµÿØŸÇÿßÿ°\".\n",
      "\n",
      "Ÿáÿ∞Ÿá ÿßŸÑÿ¨ŸÖŸÑ ÿ≥ÿ™ŸÉŸàŸÜ ŸÖŸÅŸäÿØÿ© ŸÑŸÉ ŸÅŸä ŸÅŸáŸÖ ÿßŸÑŸÅÿπŸÑ ŸàÿßŸÑŸÅÿßÿπŸÑ ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑ ÿßŸÑŸÅÿπŸÑŸäÿ©.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel  # Import Unsloth\n",
    "import faiss\n",
    "import numpy as np\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Ensure GPU is used if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type != \"cuda\":\n",
    "    print(\"Warning: CUDA not available. Using CPU, which may be slower.\")\n",
    "\n",
    "# Initialize the BGE-M3 model for embeddings\n",
    "embedding_model = BGEM3FlagModel('BAAI/bge-m3', device=device)\n",
    "\n",
    "# Load the FAISS index\n",
    "cpu_index = faiss.read_index('subject_embeddings.faiss')\n",
    "\n",
    "# Move index to GPU if available\n",
    "if device.type == \"cuda\":\n",
    "    res = faiss.StandardGpuResources()\n",
    "    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "else:\n",
    "    gpu_index = cpu_index  # Fallback to CPU index\n",
    "\n",
    "# Load metadata\n",
    "metadata_path = 'subject_metadata.pkl'\n",
    "with open(metadata_path, \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# Initialize the Qwen2.5-3B-Instruct model and tokenizer with Unsloth\n",
    "llm_model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "llm_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=llm_model_name,\n",
    "    max_seq_length=2048, # Use 4-bit quantization for memory savings\n",
    "    device_map=\"auto\"     # Automatically map to available devices\n",
    ")\n",
    "\n",
    "# Function to query FAISS without subject filter and no oversampling\n",
    "def search_faiss(query_text: str, index, embedding_model, metadata: List[Dict], top_k: int = 3) -> List[Dict]:\n",
    "    query_embedding = embedding_model.encode(query_text, max_length=256)['dense_vecs']\n",
    "    query_embedding = np.array([query_embedding]).astype('float32')\n",
    "\n",
    "    search_k = top_k\n",
    "    distances, indices = index.search(query_embedding, search_k)\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx >= len(metadata):\n",
    "            continue\n",
    "        result = {\n",
    "            'index': int(idx),\n",
    "            'distance': float(distances[0][i]),\n",
    "            'subject': metadata[idx]['subject'],\n",
    "            'text': metadata[idx]['text']\n",
    "        }\n",
    "        results.append(result)\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to call the Qwen2.5-3B-Instruct model with chat template\n",
    "def call_llm(prompt: str, max_new_tokens: int = 512) -> str:\n",
    "    # Apply chat template with enable_thinking=False\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(llm_model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generated_ids = llm_model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "    # Decode the response\n",
    "    content = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "    return content\n",
    "\n",
    "# Conversational RAG function\n",
    "def conversational_rag(query: str) -> Dict[str, str]:\n",
    "    results = search_faiss(query, gpu_index, embedding_model, metadata, top_k=3)\n",
    "    context = \"\\n\".join([f\"- {result['text']} (ŸÖŸàÿ∂Ÿàÿπ: {result['subject']})\" for result in results])\n",
    "    \n",
    "    prompt= f\"\"\"ÿßŸÜÿ™ ŸÖÿπŸÑŸÖ ÿ∞ŸÉŸä ŸàŸàÿØŸàÿØÿå Ÿàÿ∏ŸäŸÅÿ™ŸÉ ŸÖÿ≥ÿßÿπÿØÿ© ÿßŸÑÿ∑ŸÑÿßÿ® ŸÖŸÜ ÿ¨ŸÖŸäÿπ ÿßŸÑŸÖÿ±ÿßÿ≠ŸÑ ÿßŸÑÿØÿ±ÿßÿ≥Ÿäÿ© ÿßŸÑÿπŸÖÿ±Ÿäÿ© ÿπŸÜ ÿ∑ÿ±ŸäŸÇ ÿßŸÑÿ±ÿØ ÿπŸÑŸâ ÿ£ÿ≥ÿ¶ŸÑÿ™ŸáŸÖ ŸÅŸä ŸÖŸàÿßÿ∂Ÿäÿπ ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿßÿ™ÿå ÿßŸÑÿ£ÿ≠Ÿäÿßÿ°ÿå ÿßŸÑŸÉŸäŸÖŸäÿßÿ°ÿå ÿßŸÑŸÅŸäÿ≤Ÿäÿßÿ°ÿå ÿßŸÑÿ®Ÿäÿ¶ÿ©ÿå ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©ÿå ŸàÿßŸÑÿπŸÑŸàŸÖ ÿ®ÿ∑ÿ±ŸäŸÇÿ© ÿ®ÿ≥Ÿäÿ∑ÿ© Ÿàÿ≥ŸáŸÑÿ© ŸàŸàÿßÿ∂ÿ≠ÿ© ÿ™ÿ™ŸÜÿßÿ≥ÿ® ŸÖÿπ ŸÖÿ≥ÿ™ŸàÿßŸáŸÖ ÿßŸÑÿπŸÖÿ±Ÿä ŸàŸÅŸÇŸãÿß ŸÑŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖŸÇÿØŸÖ. Ÿäÿ¨ÿ® ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ŸÅŸÇÿ∑ ŸÅŸä ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ ÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿ∑ŸÑÿßÿ®ÿå ŸÖÿπ ŸÖÿ±ÿßÿπÿßÿ© ÿ™ŸÇÿØŸäŸÖ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿ®ÿ¥ŸÉŸÑ ÿØŸÇŸäŸÇ ŸàŸÖŸÜÿßÿ≥ÿ® ŸÑŸÑÿ≥ŸäÿßŸÇ.\n",
    "\n",
    "# ÿ™ÿπŸÑŸäŸÖÿßÿ™ ÿ•ÿ∂ÿßŸÅŸäÿ©\n",
    "* ÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿ£ŸÜ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿÆÿßŸÑŸäÿ© ŸÖŸÜ ÿßŸÑÿ™ÿπŸÇŸäÿØÿßÿ™ ŸàŸÖÿ®ÿßÿ¥ÿ±ÿ©.\n",
    "* ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿ≥ÿ§ÿßŸÑ Ÿäÿ™ÿ∑ŸÑÿ® ÿ¥ÿ±ÿ≠Ÿãÿß ÿπŸÑŸÖŸäŸãÿßÿå ÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÑÿ∫ÿ© ŸÖÿ®ÿ≥ÿ∑ÿ© ÿ™ŸÜÿßÿ≥ÿ® ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© ÿßŸÑÿπŸÖÿ±Ÿäÿ© ŸÑŸÑÿ∑ÿßŸÑÿ®.\n",
    "* ŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿµÿ∑ŸÑÿ≠ÿßÿ™ ŸÖÿπŸÇÿØÿ© ÿ•ŸÑÿß ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿ∂ÿ±Ÿàÿ±Ÿäÿ©ÿå ŸàŸÅŸä Ÿáÿ∞Ÿá ÿßŸÑÿ≠ÿßŸÑÿ© ÿßÿ¥ÿ±ÿ≠Ÿáÿß ÿ®ÿ®ÿ≥ÿßÿ∑ÿ©.\n",
    "\n",
    "ÿßŸÑÿ≥ÿ§ÿßŸÑ: \n",
    "{query}\n",
    "\n",
    "ÿßŸÑÿ≥ŸäÿßŸÇ:\n",
    "{context}\n",
    "\"\"\"\n",
    "    \n",
    "    response = call_llm(prompt)\n",
    "    return {\"content\": response}\n",
    "\n",
    "# Example conversational loop\n",
    "def main():\n",
    "    print(\"ŸÖÿ±ÿ≠ÿ®Ÿãÿß Ÿäÿß ÿ£ÿµÿØŸÇÿßÿ°! ÿ£ŸÜÿß ŸÖÿπŸÑŸÖ ŸàÿØŸàÿØ ŸáŸÜÿß ŸÑŸÖÿ≥ÿßÿπÿØÿ™ŸÉŸÖ ŸÅŸä ÿßŸÑÿØÿ±ÿßÿ≥ÿ© ŸÖŸÜ ÿßŸÑÿµŸÅ ÿßŸÑÿ£ŸàŸÑ ÿ•ŸÑŸâ ÿßŸÑÿµŸÅ ÿßŸÑÿ´ÿßŸÜŸä ÿπÿ¥ÿ±. ÿßÿ≥ÿ£ŸÑŸàŸÜŸä ÿ≥ÿ§ÿßŸÑŸãÿß ÿ®ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (ÿßŸÉÿ™ÿ® 'ÿÆÿ±Ÿàÿ¨' ŸÑŸÑÿÆÿ±Ÿàÿ¨):\")\n",
    "    while True:\n",
    "        query = input(\"ÿßŸÑÿ≥ÿ§ÿßŸÑ: \")\n",
    "        if query.strip() == \"ÿÆÿ±Ÿàÿ¨\":\n",
    "            break\n",
    "        result = conversational_rag(query)\n",
    "        print(f\"ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©: {result['content']}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdac018-e5ca-4086-ae63-b497f9c993fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.6.12: Fast Qwen2 patching. Transformers: 4.53.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.625 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16791/3765256397.py:46: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ŸÖÿ±ÿ≠ÿ®Ÿãÿß Ÿäÿß ÿ£ÿµÿØŸÇÿßÿ°! ÿ£ŸÜÿß ŸÖÿπŸÑŸÖ ŸàÿØŸàÿØ ŸáŸÜÿß ŸÑŸÖÿ≥ÿßÿπÿØÿ™ŸÉŸÖ ŸÅŸä ÿßŸÑÿØÿ±ÿßÿ≥ÿ© ŸÖŸÜ ÿßŸÑÿµŸÅ ÿßŸÑÿ£ŸàŸÑ ÿ•ŸÑŸâ ÿßŸÑÿµŸÅ ÿßŸÑÿ´ÿßŸÜŸä ÿπÿ¥ÿ±. ÿßÿ≥ÿ£ŸÑŸàŸÜŸä ÿ≥ÿ§ÿßŸÑŸãÿß ÿ®ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (ÿßŸÉÿ™ÿ® 'ÿÆÿ±Ÿàÿ¨' ŸÑŸÑÿÆÿ±Ÿàÿ¨):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ÿßŸÑÿ≥ÿ§ÿßŸÑ:  ŸÖÿß ŸáŸà ÿßŸÑŸÅÿπŸÑ ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑŸá ÿßŸÑŸÅÿπŸÑŸäŸá\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©: ÿ≠ÿ≥ŸÜŸãÿßÿå ÿ≥ÿ£ÿ®ÿØÿ£ ÿ®ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿ™Ÿä ŸÇÿØŸÖÿ™Ÿáÿß. ÿ≥ÿ£ŸÇŸàŸÖ ÿ®ÿ™Ÿàÿ∂Ÿäÿ≠ ÿßŸÑŸÅÿπŸÑ ŸàÿßŸÑŸÅÿßÿπŸÑ ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑ ÿßŸÑŸÅÿπŸÑŸäÿ© ÿßŸÑŸÖÿπÿ∑ÿßÿ© ŸÑŸÉÿå ÿ´ŸÖ ÿ£ŸÇÿ±ÿ£ ÿßŸÑÿ¨ŸÖŸÑ ÿ®ŸÜÿ∑ŸÇ ÿµÿ≠Ÿäÿ≠.\n",
      "\n",
      "1. \"ŸÇÿßŸÑ ÿ£ÿ®Ÿà ŸÖÿ≠ŸÖÿØ ÿßŸÑÿ≠ŸÖÿØÿßŸÜ\"\n",
      "   - ÿßŸÑŸÅÿπŸÑ: ŸÇÿßŸÑ\n",
      "   - ÿßŸÑŸÅÿßÿπŸÑ: ÿ£ÿ®Ÿà ŸÖÿ≠ŸÖÿØ ÿßŸÑÿ≠ŸÖÿØÿßŸÜ\n",
      "\n",
      "2. \"ÿ£ÿ™Ÿâ ÿßŸÑÿÆÿßÿØŸÖ ÿ•ŸÑŸâ ÿßŸÑŸÖŸÜÿ≤ŸÑ\"\n",
      "   - ÿßŸÑŸÅÿπŸÑ: ÿ£ÿ™Ÿâ\n",
      "   - ÿßŸÑŸÅÿßÿπŸÑ: ÿßŸÑÿÆÿßÿØŸÖ\n",
      "\n",
      "3. \"ÿßŸÜÿ∑ŸÅÿ£ÿ™ ÿßŸÑŸÜŸàÿ±\"\n",
      "   - ÿßŸÑŸÅÿπŸÑ: ÿßŸÜÿ∑ŸÅÿ£ÿ™\n",
      "   - ÿßŸÑŸÅÿßÿπŸÑ: ÿßŸÑŸÜŸàÿ±\n",
      "\n",
      "4. \"ÿ£ÿ≠ÿ∂ÿ±ÿ™ ÿßŸÑÿ£ŸÖ ÿßŸÑÿ∑ÿπÿßŸÖ\"\n",
      "   - ÿßŸÑŸÅÿπŸÑ: ÿ≠ÿ∂ÿ±ÿ™\n",
      "   - ÿßŸÑŸÅÿßÿπŸÑ: ÿßŸÑÿ£ŸÖ\n",
      "\n",
      "5. \"ÿßÿ≥ÿ™ŸäŸÇÿ∏ÿ™ ÿßŸÑÿ≥ŸäÿØÿ© ÿπŸÑŸâ ÿßŸÑŸÜÿßŸÇÿµ\"\n",
      "   - ÿßŸÑŸÅÿπŸÑ: ÿßÿ≥ÿ™ŸäŸÇÿ∏ÿ™\n",
      "   - ÿßŸÑŸÅÿßÿπŸÑ: ÿßŸÑÿ≥ŸäÿØÿ©\n",
      "\n",
      "ÿ£ÿ±ÿ¨Ÿà ÿ£ŸÜ ŸäŸÉŸàŸÜ Ÿáÿ∞ÿß ŸÖŸÅŸäÿØŸãÿß ŸÑŸÉ! ÿ•ÿ∞ÿß ŸÉŸÜÿ™ ÿ™ÿ≠ÿ™ÿßÿ¨ ÿ•ŸÑŸâ ÿ™Ÿàÿ∂Ÿäÿ≠ ÿ£ŸÉÿ´ÿ± ÿ£Ÿà ÿ™ÿ±ŸäÿØ ŸÖÿ≤ŸäÿØŸãÿß ŸÖŸÜ ÿßŸÑÿ£ŸÖÿ´ŸÑÿ©ÿå ŸÅŸÑÿß ÿ™ÿ™ÿ±ÿØÿØ ŸÅŸä ÿ≥ÿ§ÿßŸÑŸä.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ÿßŸÑÿ≥ÿ§ÿßŸÑ:  ŸÖÿßÿ∞ÿß ÿ®ÿπÿØ ÿßŸÑŸÅÿπŸÑ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©: ÿ≠ÿ≥ŸÜŸãÿßÿå ÿ≥ÿ£ÿ®ÿØÿ£ ÿ®ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿ™Ÿä ŸÇÿØŸÖÿ™Ÿáÿß. ÿ≥ÿ£ŸÇŸàŸÖ ÿ®ÿ™Ÿàÿ∂Ÿäÿ≠ ÿßŸÑŸÅÿπŸÑ ŸàÿßŸÑŸÅÿßÿπŸÑ ŸÅŸä ÿßŸÑÿ¨ŸÖŸÑ ÿßŸÑŸÅÿπŸÑŸäÿ© ÿßŸÑŸÖÿπÿ∑ÿßÿ© ŸÑŸÉÿå ÿ´ŸÖ ÿ£ŸÇÿ±ÿ£ ÿßŸÑÿ¨ŸÖŸÑ ÿ®ŸÜÿ∑ŸÇ ÿµÿ≠Ÿäÿ≠.\n",
      "\n",
      "1. \"ÿ¥ÿ±ŸÇÿ™ ÿ¥ŸÖÿ≥ ÿßŸÑÿπŸäÿØ ŸÅÿ®ÿßÿØÿ±ÿ™ ÿßŸÑÿ≥ÿπÿßÿØÿ© ÿ®ÿßŸÑÿ¨ŸÖŸäÿπ\"\n",
      "   - ÿßŸÑŸÅÿπŸÑ: ÿ¥ÿ±ŸÇÿ™\n",
      "   - ÿßŸÑŸÅÿßÿπŸÑ: ÿ¥ŸÖÿ≥\n",
      "   - ÿßŸÑÿ¨ŸÖŸÑÿ© ÿßŸÑŸÅÿπŸÑŸäÿ©: ÿ¥ÿ±ŸÇÿ™ ÿ¥ŸÖÿ≥\n",
      "\n",
      "2. \"ÿ∏Ÿáÿ±ÿ™ ŸÖÿ∏ÿßŸáÿ± ÿßŸÑÿπŸäÿØ ÿπŸÑŸâ ŸÉŸÑ ÿßŸÑŸàÿ¨ŸàŸá\"\n",
      "   - ÿßŸÑŸÅÿπŸÑ: ÿ∏Ÿáÿ±ÿ™\n",
      "   - ÿßŸÑŸÅÿßÿπŸÑ: ÿßŸÑÿπŸäÿØ\n",
      "   - ÿßŸÑÿ¨ŸÖŸÑÿ© ÿßŸÑŸÅÿπŸÑŸäÿ©: ÿ∏Ÿáÿ±ÿ™ ŸÖÿ∏ÿßŸáÿ± ÿßŸÑÿπŸäÿØ\n",
      "\n",
      "3. \"ŸÅŸä ŸÉŸÑ ÿßŸÑÿ®ŸäŸàÿ™ ŸàÿßŸÑÿ¥Ÿàÿßÿ±ÿπ ÿ™ÿ®ÿ™ÿ≥ŸÖ ÿßŸÑÿ£ŸÅŸàÿßŸá Ÿàÿ™ÿ±ÿØÿØ ÿßŸÑÿ£ŸÑÿ≥ŸÜ ÿßŸÑÿπÿ®ÿßÿ±ÿßÿ™ ÿßŸÑÿ™ŸáŸÜÿ¶ÿ©\"\n",
      "   - ÿßŸÑŸÅÿπŸÑ: ÿ™ÿ®ÿ™ÿ≥ŸÖ\n",
      "   - ÿßŸÑŸÅÿßÿπŸÑ: ÿßŸÑÿ£ŸÅŸàÿßŸá\n",
      "   - ÿßŸÑÿ¨ŸÖŸÑÿ© ÿßŸÑŸÅÿπŸÑŸäÿ©: ÿ™ÿ®ÿ™ÿ≥ŸÖ ÿßŸÑÿ£ŸÅŸàÿßŸá\n",
      "\n",
      "4. \"ÿßÿ±ÿ™ÿØŸâ ÿßŸÑÿµÿ∫Ÿäÿ± ŸàÿßŸÑŸÉÿ®Ÿäÿ± ÿßŸÑŸÖŸÑÿßÿ®ÿ≥ ÿßŸÑÿ¨ÿØŸäÿØÿ©\"\n",
      "   - ÿßŸÑŸÅÿπŸÑ: ÿßÿ±ÿ™ÿØŸâ\n",
      "   - ÿßŸÑŸÅÿßÿπŸÑ: ÿßŸÑÿµÿ∫Ÿäÿ± ŸàÿßŸÑŸÉÿ®Ÿäÿ±\n",
      "   - ÿßŸÑÿ¨ŸÖŸÑÿ© ÿßŸÑŸÅÿπŸÑŸäÿ©: ÿßÿ±ÿ™ÿØŸâ ÿßŸÑÿµÿ∫Ÿäÿ± ŸàÿßŸÑŸÉÿ®Ÿäÿ±\n",
      "\n",
      "5. \"ÿ≠ÿµŸÑ ÿßŸÑÿ£ÿ∑ŸÅÿßŸÑ ÿπŸÑŸâ ÿßŸÑÿπŸäÿØŸäÿßÿ™\"\n",
      "   - ÿßŸÑŸÅÿπŸÑ: ÿ≠ÿµŸÑ\n",
      "   - ÿßŸÑŸÅÿßÿπŸÑ: ÿßŸÑÿ£ÿ∑ŸÅÿßŸÑ\n",
      "   - ÿßŸÑÿ¨ŸÖŸÑÿ© ÿßŸÑŸÅÿπŸÑŸäÿ©: ÿ≠ÿµŸÑ ÿßŸÑÿ£ÿ∑ŸÅÿßŸÑ\n",
      "\n",
      "ÿ£ÿ±ÿ¨Ÿà ÿ£ŸÜ ŸäŸÉŸàŸÜ Ÿáÿ∞ÿß ŸÖŸÅŸäÿØŸãÿß ŸÑŸÉ! ÿ•ÿ∞ÿß ŸÉŸÜÿ™ ÿ™ÿ≠ÿ™ÿßÿ¨ ÿ•ŸÑŸâ ÿ™Ÿàÿ∂Ÿäÿ≠ ÿ£ŸÉÿ´ÿ± ÿ£Ÿà ÿ™ÿ±ŸäÿØ ŸÖÿ≤ŸäÿØŸãÿß ŸÖŸÜ ÿßŸÑÿ£ŸÖÿ´ŸÑÿ©ÿå ŸÅŸÑÿß ÿ™ÿ™ÿ±ÿØÿØ ŸÅŸä ÿ≥ÿ§ÿßŸÑŸä.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "import faiss\n",
    "import numpy as np\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict\n",
    "import pickle\n",
    "import os\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Ensure GPU is used if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type != \"cuda\":\n",
    "    print(\"Warning: CUDA not available. Using CPU, which may be slower.\")\n",
    "\n",
    "# Initialize the BGE-M3 model for embeddings\n",
    "embedding_model = BGEM3FlagModel('BAAI/bge-m3', device=device)\n",
    "\n",
    "# Load the FAISS index\n",
    "cpu_index = faiss.read_index('subject_embeddings.faiss')\n",
    "\n",
    "# Move index to GPU if available\n",
    "if device.type == \"cuda\":\n",
    "    res = faiss.StandardGpuResources()\n",
    "    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "else:\n",
    "    gpu_index = cpu_index  # Fallback to CPU index\n",
    "\n",
    "# Load metadata\n",
    "metadata_path = 'subject_metadata.pkl'\n",
    "with open(metadata_path, \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# Initialize the Qwen2.5-3B-Instruct model and tokenizer with Unsloth\n",
    "llm_model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "llm_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=llm_model_name,\n",
    "    max_seq_length=2048,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Initialize LangChain memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"query\",\n",
    "    output_key=\"content\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Define the prompt template with memory\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"query\", \"context\"],\n",
    "    template=\"\"\"ÿßŸÜÿ™ ŸÖÿπŸÑŸÖ ÿ∞ŸÉŸä ŸàŸàÿØŸàÿØÿå Ÿàÿ∏ŸäŸÅÿ™ŸÉ ŸÖÿ≥ÿßÿπÿØÿ© ÿßŸÑÿ∑ŸÑÿßÿ® ŸÖŸÜ ÿ¨ŸÖŸäÿπ ÿßŸÑŸÖÿ±ÿßÿ≠ŸÑ ÿßŸÑÿØÿ±ÿßÿ≥Ÿäÿ© ÿßŸÑÿπŸÖÿ±Ÿäÿ© ÿπŸÜ ÿ∑ÿ±ŸäŸÇ ÿßŸÑÿ±ÿØ ÿπŸÑŸâ ÿ£ÿ≥ÿ¶ŸÑÿ™ŸáŸÖ ŸÅŸä ŸÖŸàÿßÿ∂Ÿäÿπ ÿßŸÑÿ±Ÿäÿßÿ∂Ÿäÿßÿ™ÿå ÿßŸÑÿ£ÿ≠Ÿäÿßÿ°ÿå ÿßŸÑŸÉŸäŸÖŸäÿßÿ°ÿå ÿßŸÑŸÅŸäÿ≤Ÿäÿßÿ°ÿå ÿßŸÑÿ®Ÿäÿ¶ÿ©ÿå ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©ÿå ŸàÿßŸÑÿπŸÑŸàŸÖ ÿ®ÿ∑ÿ±ŸäŸÇÿ© ÿ®ÿ≥Ÿäÿ∑ÿ© Ÿàÿ≥ŸáŸÑÿ© ŸàŸàÿßÿ∂ÿ≠ÿ© ÿ™ÿ™ŸÜÿßÿ≥ÿ® ŸÖÿπ ŸÖÿ≥ÿ™ŸàÿßŸáŸÖ ÿßŸÑÿπŸÖÿ±Ÿä ŸàŸÅŸÇŸãÿß ŸÑŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖŸÇÿØŸÖ. Ÿäÿ¨ÿ® ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ŸÅŸÇÿ∑ ŸÅŸä ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿπŸÑŸâ ÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿ∑ŸÑÿßÿ®ÿå ŸÖÿπ ŸÖÿ±ÿßÿπÿßÿ© ÿ™ŸÇÿØŸäŸÖ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿ®ÿ¥ŸÉŸÑ ÿØŸÇŸäŸÇ ŸàŸÖŸÜÿßÿ≥ÿ® ŸÑŸÑÿ≥ŸäÿßŸÇ.\n",
    "\n",
    "# ÿ™ÿπŸÑŸäŸÖÿßÿ™ ÿ•ÿ∂ÿßŸÅŸäÿ©\n",
    "* ÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿ£ŸÜ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿÆÿßŸÑŸäÿ© ŸÖŸÜ ÿßŸÑÿ™ÿπŸÇŸäÿØÿßÿ™ ŸàŸÖÿ®ÿßÿ¥ÿ±ÿ©.\n",
    "* ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿßŸÑÿ≥ÿ§ÿßŸÑ Ÿäÿ™ÿ∑ŸÑÿ® ÿ¥ÿ±ÿ≠Ÿãÿß ÿπŸÑŸÖŸäŸãÿßÿå ÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÑÿ∫ÿ© ŸÖÿ®ÿ≥ÿ∑ÿ© ÿ™ŸÜÿßÿ≥ÿ® ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© ÿßŸÑÿπŸÖÿ±Ÿäÿ© ŸÑŸÑÿ∑ÿßŸÑÿ®.\n",
    "* ŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿµÿ∑ŸÑÿ≠ÿßÿ™ ŸÖÿπŸÇÿØÿ© ÿ•ŸÑÿß ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿ∂ÿ±Ÿàÿ±Ÿäÿ©ÿå ŸàŸÅŸä Ÿáÿ∞Ÿá ÿßŸÑÿ≠ÿßŸÑÿ© ÿßÿ¥ÿ±ÿ≠Ÿáÿß ÿ®ÿ®ÿ≥ÿßÿ∑ÿ©.\n",
    "* ÿ•ÿ∞ÿß ŸÉÿßŸÜ ŸáŸÜÿßŸÉ ÿ≥ÿ¨ŸÑ ŸÖÿ≠ÿßÿØÿ´ÿ© ÿ≥ÿßÿ®ŸÇÿå ÿßÿ≥ÿ™ÿÆÿØŸÖŸá ŸÑÿ∂ŸÖÿßŸÜ ÿßÿ≥ÿ™ŸÖÿ±ÿßÿ±Ÿäÿ© ÿßŸÑÿ≥ŸäÿßŸÇ.\n",
    "\n",
    "ÿ≥ÿ¨ŸÑ ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ©:\n",
    "{chat_history}\n",
    "\n",
    "ÿßŸÑÿ≥ÿ§ÿßŸÑ: \n",
    "{query}\n",
    "\n",
    "ÿßŸÑÿ≥ŸäÿßŸÇ:\n",
    "{context}\"\"\"\n",
    ")\n",
    "\n",
    "# Function to query FAISS without subject filter and no oversampling\n",
    "def search_faiss(query_text: str, index, embedding_model, metadata: List[Dict], top_k: int = 3) -> List[Dict]:\n",
    "    query_embedding = embedding_model.encode(query_text, max_length=256)['dense_vecs']\n",
    "    query_embedding = np.array([query_embedding]).astype('float32')\n",
    "\n",
    "    search_k = top_k\n",
    "    distances, indices = index.search(query_embedding, search_k)\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx >= len(metadata):\n",
    "            continue\n",
    "        result = {\n",
    "            'index': int(idx),\n",
    "            'distance': float(distances[0][i]),\n",
    "            'subject': metadata[idx]['subject'],\n",
    "            'text': metadata[idx]['text']\n",
    "        }\n",
    "        results.append(result)\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to call the Qwen25-3B-Instruct model with chat template\n",
    "def call_llm(prompt: str, max_new_tokens: int = 512) -> str:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(llm_model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generated_ids = llm_model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "    # Decode the response\n",
    "    content = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "    return content\n",
    "\n",
    "# Conversational RAG function with memory\n",
    "def conversational_rag(query: str) -> Dict[str, str]:\n",
    "    # Search FAISS for relevant context\n",
    "    results = search_faiss(query, gpu_index, embedding_model, metadata, top_k=3)\n",
    "    context = \"\\n\".join([f\"- {result['text']} (ŸÖŸàÿ∂Ÿàÿπ: {result['subject']})\" for result in results])\n",
    "\n",
    "    # Get chat history from memory\n",
    "    chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "    \n",
    "    # Format the prompt with history, query, and context\n",
    "    prompt = prompt_template.format(\n",
    "        chat_history=chat_history,\n",
    "        query=query,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "    # Generate response\n",
    "    response = call_llm(prompt)\n",
    "\n",
    "    # Save the query and response to memory\n",
    "    memory.save_context({\"query\": query}, {\"content\": response})\n",
    "\n",
    "    return {\"content\": response}\n",
    "\n",
    "# Example conversational loop\n",
    "def main():\n",
    "    print(\"ŸÖÿ±ÿ≠ÿ®Ÿãÿß Ÿäÿß ÿ£ÿµÿØŸÇÿßÿ°! ÿ£ŸÜÿß ŸÖÿπŸÑŸÖ ŸàÿØŸàÿØ ŸáŸÜÿß ŸÑŸÖÿ≥ÿßÿπÿØÿ™ŸÉŸÖ ŸÅŸä ÿßŸÑÿØÿ±ÿßÿ≥ÿ© ŸÖŸÜ ÿßŸÑÿµŸÅ ÿßŸÑÿ£ŸàŸÑ ÿ•ŸÑŸâ ÿßŸÑÿµŸÅ ÿßŸÑÿ´ÿßŸÜŸä ÿπÿ¥ÿ±. ÿßÿ≥ÿ£ŸÑŸàŸÜŸä ÿ≥ÿ§ÿßŸÑŸãÿß ÿ®ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (ÿßŸÉÿ™ÿ® 'ÿÆÿ±Ÿàÿ¨' ŸÑŸÑÿÆÿ±Ÿàÿ¨):\")\n",
    "    while True:\n",
    "        query = input(\"ÿßŸÑÿ≥ÿ§ÿßŸÑ: \")\n",
    "        if query.strip() == \"ÿÆÿ±Ÿàÿ¨\":\n",
    "            break\n",
    "        result = conversational_rag(query)\n",
    "        print(f\"ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©: {result['content']}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88f70c-cc54-4191-98a0-d294b23f487a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
