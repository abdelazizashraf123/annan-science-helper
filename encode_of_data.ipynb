{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e3660f-bc74-41a4-b63a-9adcf33e895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # List of CSV files to merge\n",
    "# csv_files = ['dares/test.csv', 'dares/train.csv', 'dares/validation.csv']\n",
    "\n",
    "# # Read and concatenate all CSV files\n",
    "# merged_df = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "# # Save the merged dataframe to a new CSV file\n",
    "# merged_df.to_csv('merged_output.csv', index=False)\n",
    "\n",
    "# print(\"CSV files merged successfully into 'merged_output.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93429a19-5f2a-415a-940e-72859da0369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35aff5a-7053-4235-93aa-d3a6f3b42fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merged_output.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9cd9b2b-3b42-4308-9e87-92635f2147ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_math = df[df[\"English_Filename\"]==\"Math\"].reset_index(drop=True)\n",
    "df_Science = df[df[\"English_Filename\"]==\"Science\"].reset_index(drop=True)\n",
    "df_Physics = df[df[\"English_Filename\"]==\"Physics\"].reset_index(drop=True)\n",
    "df_Ecology = df[df[\"English_Filename\"]==\"Ecology\"].reset_index(drop=True)\n",
    "df_Chemistry = df[df[\"English_Filename\"]==\"Chemistry\"].reset_index(drop=True)\n",
    "df_Biology = df[df[\"English_Filename\"]==\"Biology\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de16723-25f6-481b-b31b-365d7b580b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Arabic1 = df[df[\"English_Filename\"]==\"ArabicLanguage\"].reset_index(drop=True)\n",
    "df_Arabic2 = df[df[\"English_Filename\"]==\"Arabic_Language\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "278c1ff5-d9ca-4f76-8aba-c313bedacfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Arabic = pd.concat([df_Arabic1,df_Arabic2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1636c511-9148-45af-a259-3547049c542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "# import numpy as np\n",
    "# from FlagEmbedding import BGEM3FlagModel\n",
    "# import torch\n",
    "# from tqdm.notebook import tqdm\n",
    "# import pickle\n",
    "\n",
    "# # Ensure GPU is used if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if device.type != \"cuda\":\n",
    "#     raise RuntimeError(\"CUDA device not available. Ensure GPU and faiss-gpu are properly installed.\")\n",
    "\n",
    "# # Initialize the BGE-M3 model\n",
    "# model = BGEM3FlagModel('BAAI/bge-m3', device=device)\n",
    "\n",
    "# # Initialize FAISS GPU resources and index\n",
    "# dimension = 1024  # BGE-M3 embedding dimension\n",
    "# res = faiss.StandardGpuResources()  # Create GPU resources\n",
    "# index = faiss.IndexFlatL2(dimension)  # Create CPU index\n",
    "# gpu_index = faiss.index_cpu_to_gpu(res, 0, index)  # Move index to GPU (device 0)\n",
    "\n",
    "# # List to store metadata\n",
    "# metadata = []\n",
    "\n",
    "# # Function to process and add embeddings for a given dataframe\n",
    "# def add_embeddings_to_index(df, subject_name, index, metadata_list):\n",
    "#     embeddings = []\n",
    "#     for i in tqdm(range(len(df)), desc=f\"Encoding {subject_name}\"):\n",
    "#         emb = model.encode(df[\"Text\"][i], max_length=256)['dense_vecs']\n",
    "#         embeddings.append(emb)\n",
    "        \n",
    "#         # Store only text and subject as metadata\n",
    "#         meta = {'subject': subject_name, 'text': df[\"Text\"][i]}\n",
    "#         metadata_list.append(meta)\n",
    "    \n",
    "#     # Convert embeddings to numpy array and add to FAISS GPU index\n",
    "#     embeddings = np.array(embeddings).astype('float32')\n",
    "#     index.add(embeddings)\n",
    "#     return embeddings\n",
    "\n",
    "# # Process each subject's dataframe\n",
    "# subjects = [\n",
    "#     ('Math', df_math),\n",
    "#     ('Arabic', df_Arabic),\n",
    "#     ('Science', df_Science),\n",
    "#     ('Physics', df_Physics),\n",
    "#     ('Ecology', df_Ecology),\n",
    "#     ('Chemistry', df_Chemistry),\n",
    "#     ('Biology', df_Biology)\n",
    "# ]\n",
    "\n",
    "# for subject_name, df in subjects:\n",
    "#     add_embeddings_to_index(df, subject_name, gpu_index, metadata)\n",
    "\n",
    "# # Move index back to CPU for saving (optional, if you need CPU compatibility)\n",
    "# cpu_index = faiss.index_gpu_to_cpu(gpu_index)\n",
    "# faiss.write_index(cpu_index, \"subject_embeddings.faiss\")\n",
    "\n",
    "# # Save metadata\n",
    "# with open(\"subject_metadata.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(metadata, f)\n",
    "\n",
    "# print(f\"FAISS GPU index created with {gpu_index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a6b769-c12e-420a-80bf-d1e6591c780e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9f471a4888424eb8bd48db01dd7c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ما هو الفعل\n",
      "Subject Filter: None\n",
      "Top 5 results (up to 5):\n",
      "\n",
      "Result 1:\n",
      "Subject: Arabic\n",
      "Text: الجملة الفعلية تتألف من ركنين أساسيين الفعل والفاعل. الفاعل هو اسم مرفوع يدل على الشخص القائم بالفعل. علامة رفع الفاعل هي الضمة التي تظهر على الحرف الأخير. أضع الأسماء التالية في جملة تكون فيها فاعلا، ثم أرددها بنطق سليم. أضع فاعلا لكل فعل من الأفعال التالية ثم أرددها بتلاوة سليمة. اقرأ الفقرة التالية، وعرف الفعل والفاعل، ونطقهما نطقا سليما شرقت شمس العيد فبادرت السعادة بالجميع، وظهرت مظاهر العيد على كل الوجوه، وفي كل البيوت والشوارع تبتسم الأفواه وتردد الألسن العبارات التهنئة. يرتدي الصغير والكبير الملابس الجديدة، ويحصل الأطفال على العيديات. أجب عن الأسئلة التالية بجملة فعلية واقرأها بالضبط الصحيح ماذا فعل التلميذ في الدرس؟ أجعل الفاعل مفردا في الجمل التالية وضبطها بالشكل طاف الحاج حول الكعبة. أجعل الفاعل جمعا في الجمل التالية وضبطها بالشكل انتصر الجنود على الأعداء. يلعب الأولاد بالكرة.\n",
      "Distance: 0.7882\n",
      "\n",
      "Result 2:\n",
      "Subject: Arabic\n",
      "Text: واستبدل الكلمات غير المستعملة أو غير المفهومة أو المكررة بما يناسب السياق، واحذف كلمة الشكل وكلمة الرقم. والفاعل هو الاسم المرفوع الذي يلي الفعل للدلالة على من نفذ الفعل. في الجملة يجلس خالد، كلمة خالد هي اسم مرفوع وتأتي بعد الفعل يجلس للدلالة على الشخص الذي قام بالجلوس. وإذا لم يعرف الفاعل أو لم يرد ذكره، يبنى الفعل دون تحديد فاعله بتغيير صياغته، حيث يضاف اسم المفعول أولا في الماضي ويحذف ما قبل آخره، مثل قولنا ظلمت وظلم. ويضاف اسم المفعول أولا في المضارع ويفتح ما قبل آخره، مثل قولنا يظلم ويظلم. وهنا يصبح للفعل نائب فاعل مرفوع، مثل في الجملة ظلم السجين، حيث يدل على الشخص القام بعملية الظلم.\n",
      "Distance: 0.8973\n",
      "\n",
      "Result 3:\n",
      "Subject: Arabic\n",
      "Text: صحيح الإملاء والكلمات في النصوص، واستبدل الكلمات غير المناسبة بما يناسب السياق. الفاعل هو الذي يقوم بأداء الفعل، وهو ركن أساسي في الجملة. الشكر لله دائما من سمات المؤمنين. الطالبة جادة في دراستها. الجملة الفعلية تتكون من الفاعل والمفعول، وهما ركنان أساسيان. الخبر والفاعل يكونان في حالة الرفع دائما. الأسماء قد تكون في حالة الرفع أو النصب أو الجر. الجملة الاسمية تحوي الركن الثاني الضروري لتكتمل معانيها. الهمزة لا تنطق في بداية الكلام. الكلمات المتعددة قد تكون لها مفهوم واحد. من المشتقات دالة المفعول، وهي تأتي من الفعل الأصلي. دعاء المظلوم غالبا ما يستجاب.\n",
      "Distance: 0.9415\n",
      "\n",
      "Result 4:\n",
      "Subject: Arabic\n",
      "Text: أنا طفل يحب اللعب على أطراف أصابعه، ووالدي بناء شامخ للأركان. الفعل المضارع يترافق مع فعل معرب. الفعلان الماضي والأمر يتميزان بوجود إحدى الحروف. يتعامل المجتمع مع المسنين بكل تواضع واحترام لنجعلهم يكونون إضافة إلى المجموع. الفعل المضارع يصنف حسب نهايته وحالته الإعرابية، وعلامة إعرابه حرف السين أو سوف، ويمكن أن تضاف إليه حروف النصب، كما في سأخرج، سوف أخرج.\n",
      "Distance: 0.9419\n",
      "\n",
      "Result 5:\n",
      "Subject: Arabic\n",
      "Text: إن الوطن عزيز على أفراده، وهذا حقيقة لا يمكن إنكارها. الفعل المضارع إذا انتهى بآخره ألف اثنان أو واو الجماعة له صورتان إما يبدأ بياء الغائب أو تاء تفعلان. أما الفعل المتصل بواو الجماعة فيكون له صورتان إما يبدأ بالياء أو التاء كما في حالة المخاطبة. لم يكن يمكن أن يبدأ إلا بتاء. الذين يحبون وطنهم يحبونه بصدق وإخلاص.\n",
      "Distance: 0.9542\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure GPU is used if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type != \"cuda\":\n",
    "    raise RuntimeError(\"CUDA device not available. Ensure GPU and faiss-gpu are properly installed.\")\n",
    "\n",
    "# Initialize the BGE-M3 model\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', device=device)\n",
    "\n",
    "# Load the FAISS index\n",
    "cpu_index = faiss.read_index(\"subject_embeddings.faiss\")\n",
    "\n",
    "# Move index to GPU\n",
    "res = faiss.StandardGpuResources()\n",
    "gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "\n",
    "# Load metadata\n",
    "with open(\"subject_metadata.pkl\", \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# Function to query the FAISS index with subject filter\n",
    "def search_faiss_with_subject_filter(query_text, index, model, metadata, subject_filter=None, top_k=5):\n",
    "    # Encode the query text\n",
    "    query_embedding = model.encode(query_text, max_length=256)['dense_vecs']\n",
    "    query_embedding = np.array([query_embedding]).astype('float32')  # Shape: (1, dimension)\n",
    "\n",
    "    # Perform similarity search (retrieve more results to account for filtering)\n",
    "    search_k = top_k * 10  # Oversample to ensure enough results after filtering\n",
    "    distances, indices = index.search(query_embedding, search_k)\n",
    "\n",
    "    # Filter results by subject and collect top-k\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx >= len(metadata):  # Safety check for index bounds\n",
    "            continue\n",
    "        meta = metadata[idx]\n",
    "        # Apply subject filter (case-insensitive)\n",
    "        if subject_filter is None or meta['subject'].lower() == subject_filter.lower():\n",
    "            result = {\n",
    "                'index': int(idx),\n",
    "                'distance': float(distances[0][i]),\n",
    "                'subject': meta['subject'],\n",
    "                'text': meta['text']\n",
    "            }\n",
    "            results.append(result)\n",
    "        if len(results) >= top_k:  # Stop once we have enough filtered results\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example query with subject filter\n",
    "query = \"ما هو الفعل\"\n",
    "subject_filter = None  # Set to None for no filtering, or specify a subject like \"Math\"\n",
    "top_k = 5  # Number of top results to retrieve\n",
    "results = search_faiss_with_subject_filter(query, gpu_index, model, metadata, subject_filter, top_k)\n",
    "\n",
    "# Print results\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Subject Filter: {subject_filter if subject_filter else 'None'}\")\n",
    "print(f\"Top {len(results)} results (up to {top_k}):\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"Subject: {result['subject']}\")\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Distance: {result['distance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8ef10e-1dc8-43ff-9ab0-8abeca2aeb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.6.12: Fast Qwen2 patching. Transformers: 4.53.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.625 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "مرحبًا يا أصدقاء! أنا معلم ودود هنا لمساعدتكم في الدراسة من الصف الأول إلى الصف الثاني عشر. اسألوني سؤالًا بالعربية (اكتب 'خروج' للخروج):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "السؤال:  ما هو الفعل في الجمله الفعليه\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الإجابة: بالطبع، سأساعدك في هذا السؤال. في الجملة \"قالت زهرة كلمة حلوة\" الفعل هنا هو \"قالت\" وهو فعل مضارع مبني على الفتح، والفاعل هنا هو \"زهرة\". \n",
      "\n",
      "في الجملة \"أعطى أحمد كتابه لأمها\" الفعل هنا هو \"أعطى\" وهو فعل ماضٍ مبني على الفتح، والفاعل هنا هو \"أحمد\".\n",
      "\n",
      "في الجملة \"تذاخر الأمانات في القلوب\" الفعل هنا هو \"تذاخر\" وهو فعل ماضٍ مبني على الفتح، والفاعل هنا هو \"الأمانات\".\n",
      "\n",
      "في الجملة \"تسللت النجوم إلى السماء\" الفعل هنا هو \"تسللت\" وهو فعل ماضٍ مبني على الفتح، والفاعل هنا هو \"النجوم\".\n",
      "\n",
      "في الجملة \"أقبلت الشمس على الأرض\" الفعل هنا هو \"أقبلت\" وهو فعل ماضٍ مبني على الفتح، والفاعل هنا هو \"الشمس\".\n",
      "\n",
      "في الجملة \"تجمعت الحشرات حول الطعام\" الفعل هنا هو \"تجمعت\" وهو فعل ماضٍ مبني على الفتح، والفاعل هنا هو \"الحشرات\".\n",
      "\n",
      "في الجملة \"تعددت الأرقام في الحساب\" الفعل هنا هو \"تعددت\" وهو فعل ماضٍ مبني على الفتح، والفاعل هنا هو \"الأرقام\".\n",
      "\n",
      "في الجملة \"تبلورت الرغبات في الأحلام\" الفعل هنا هو \"تبلورت\" وهو فعل ماضٍ مبني على الفتح، والفاعل هنا هو \"الرغبات\".\n",
      "\n",
      "في الجملة \"تكتسبت الثقة في النفس\" الفعل هنا هو \"تكتسبت\" وهو فعل ماضٍ مبني على الفتح، والفاعل هنا هو \"الثقة\".\n",
      "\n",
      "في الجملة \"تتوافدت الأصدقاء إلى المنزل\" الفعل هنا هو \"تتوافدت\" وهو فعل ماضٍ مبني على الفتح، والفاعل هنا هو \"الأصدقاء\".\n",
      "\n",
      "هذه الجمل ستكون مفيدة لك في فهم الفعل والفاعل في الجمل الفعلية.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel  # Import Unsloth\n",
    "import faiss\n",
    "import numpy as np\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Ensure GPU is used if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type != \"cuda\":\n",
    "    print(\"Warning: CUDA not available. Using CPU, which may be slower.\")\n",
    "\n",
    "# Initialize the BGE-M3 model for embeddings\n",
    "embedding_model = BGEM3FlagModel('BAAI/bge-m3', device=device)\n",
    "\n",
    "# Load the FAISS index\n",
    "cpu_index = faiss.read_index('subject_embeddings.faiss')\n",
    "\n",
    "# Move index to GPU if available\n",
    "if device.type == \"cuda\":\n",
    "    res = faiss.StandardGpuResources()\n",
    "    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "else:\n",
    "    gpu_index = cpu_index  # Fallback to CPU index\n",
    "\n",
    "# Load metadata\n",
    "metadata_path = 'subject_metadata.pkl'\n",
    "with open(metadata_path, \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# Initialize the Qwen2.5-3B-Instruct model and tokenizer with Unsloth\n",
    "llm_model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "llm_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=llm_model_name,\n",
    "    max_seq_length=2048, # Use 4-bit quantization for memory savings\n",
    "    device_map=\"auto\"     # Automatically map to available devices\n",
    ")\n",
    "\n",
    "# Function to query FAISS without subject filter and no oversampling\n",
    "def search_faiss(query_text: str, index, embedding_model, metadata: List[Dict], top_k: int = 3) -> List[Dict]:\n",
    "    query_embedding = embedding_model.encode(query_text, max_length=256)['dense_vecs']\n",
    "    query_embedding = np.array([query_embedding]).astype('float32')\n",
    "\n",
    "    search_k = top_k\n",
    "    distances, indices = index.search(query_embedding, search_k)\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx >= len(metadata):\n",
    "            continue\n",
    "        result = {\n",
    "            'index': int(idx),\n",
    "            'distance': float(distances[0][i]),\n",
    "            'subject': metadata[idx]['subject'],\n",
    "            'text': metadata[idx]['text']\n",
    "        }\n",
    "        results.append(result)\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to call the Qwen2.5-3B-Instruct model with chat template\n",
    "def call_llm(prompt: str, max_new_tokens: int = 512) -> str:\n",
    "    # Apply chat template with enable_thinking=False\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(llm_model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generated_ids = llm_model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "    # Decode the response\n",
    "    content = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "    return content\n",
    "\n",
    "# Conversational RAG function\n",
    "def conversational_rag(query: str) -> Dict[str, str]:\n",
    "    results = search_faiss(query, gpu_index, embedding_model, metadata, top_k=3)\n",
    "    context = \"\\n\".join([f\"- {result['text']} (موضوع: {result['subject']})\" for result in results])\n",
    "    \n",
    "    prompt= f\"\"\"انت معلم ذكي وودود، وظيفتك مساعدة الطلاب من جميع المراحل الدراسية العمرية عن طريق الرد على أسئلتهم في مواضيع الرياضيات، الأحياء، الكيمياء، الفيزياء، البيئة، اللغة العربية، والعلوم بطريقة بسيطة وسهلة وواضحة تتناسب مع مستواهم العمري وفقًا للسياق المقدم. يجب استخدام اللغة العربية فقط في الإجابة على أسئلة الطلاب، مع مراعاة تقديم الإجابات بشكل دقيق ومناسب للسياق.\n",
    "\n",
    "# تعليمات إضافية\n",
    "* تأكد من أن الإجابة خالية من التعقيدات ومباشرة.\n",
    "* إذا كان السؤال يتطلب شرحًا علميًا، استخدم لغة مبسطة تناسب المرحلة العمرية للطالب.\n",
    "* لا تستخدم مصطلحات معقدة إلا إذا كانت ضرورية، وفي هذه الحالة اشرحها ببساطة.\n",
    "\n",
    "السؤال: \n",
    "{query}\n",
    "\n",
    "السياق:\n",
    "{context}\n",
    "\"\"\"\n",
    "    \n",
    "    response = call_llm(prompt)\n",
    "    return {\"content\": response}\n",
    "\n",
    "# Example conversational loop\n",
    "def main():\n",
    "    print(\"مرحبًا يا أصدقاء! أنا معلم ودود هنا لمساعدتكم في الدراسة من الصف الأول إلى الصف الثاني عشر. اسألوني سؤالًا بالعربية (اكتب 'خروج' للخروج):\")\n",
    "    while True:\n",
    "        query = input(\"السؤال: \")\n",
    "        if query.strip() == \"خروج\":\n",
    "            break\n",
    "        result = conversational_rag(query)\n",
    "        print(f\"الإجابة: {result['content']}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdac018-e5ca-4086-ae63-b497f9c993fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.6.12: Fast Qwen2 patching. Transformers: 4.53.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.625 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16791/3765256397.py:46: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مرحبًا يا أصدقاء! أنا معلم ودود هنا لمساعدتكم في الدراسة من الصف الأول إلى الصف الثاني عشر. اسألوني سؤالًا بالعربية (اكتب 'خروج' للخروج):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "السؤال:  ما هو الفعل في الجمله الفعليه\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الإجابة: حسنًا، سأبدأ بالأسئلة التي قدمتها. سأقوم بتوضيح الفعل والفاعل في الجمل الفعلية المعطاة لك، ثم أقرأ الجمل بنطق صحيح.\n",
      "\n",
      "1. \"قال أبو محمد الحمدان\"\n",
      "   - الفعل: قال\n",
      "   - الفاعل: أبو محمد الحمدان\n",
      "\n",
      "2. \"أتى الخادم إلى المنزل\"\n",
      "   - الفعل: أتى\n",
      "   - الفاعل: الخادم\n",
      "\n",
      "3. \"انطفأت النور\"\n",
      "   - الفعل: انطفأت\n",
      "   - الفاعل: النور\n",
      "\n",
      "4. \"أحضرت الأم الطعام\"\n",
      "   - الفعل: حضرت\n",
      "   - الفاعل: الأم\n",
      "\n",
      "5. \"استيقظت السيدة على الناقص\"\n",
      "   - الفعل: استيقظت\n",
      "   - الفاعل: السيدة\n",
      "\n",
      "أرجو أن يكون هذا مفيدًا لك! إذا كنت تحتاج إلى توضيح أكثر أو تريد مزيدًا من الأمثلة، فلا تتردد في سؤالي.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "السؤال:  ماذا بعد الفعل\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الإجابة: حسنًا، سأبدأ بالأسئلة التي قدمتها. سأقوم بتوضيح الفعل والفاعل في الجمل الفعلية المعطاة لك، ثم أقرأ الجمل بنطق صحيح.\n",
      "\n",
      "1. \"شرقت شمس العيد فبادرت السعادة بالجميع\"\n",
      "   - الفعل: شرقت\n",
      "   - الفاعل: شمس\n",
      "   - الجملة الفعلية: شرقت شمس\n",
      "\n",
      "2. \"ظهرت مظاهر العيد على كل الوجوه\"\n",
      "   - الفعل: ظهرت\n",
      "   - الفاعل: العيد\n",
      "   - الجملة الفعلية: ظهرت مظاهر العيد\n",
      "\n",
      "3. \"في كل البيوت والشوارع تبتسم الأفواه وتردد الألسن العبارات التهنئة\"\n",
      "   - الفعل: تبتسم\n",
      "   - الفاعل: الأفواه\n",
      "   - الجملة الفعلية: تبتسم الأفواه\n",
      "\n",
      "4. \"ارتدى الصغير والكبير الملابس الجديدة\"\n",
      "   - الفعل: ارتدى\n",
      "   - الفاعل: الصغير والكبير\n",
      "   - الجملة الفعلية: ارتدى الصغير والكبير\n",
      "\n",
      "5. \"حصل الأطفال على العيديات\"\n",
      "   - الفعل: حصل\n",
      "   - الفاعل: الأطفال\n",
      "   - الجملة الفعلية: حصل الأطفال\n",
      "\n",
      "أرجو أن يكون هذا مفيدًا لك! إذا كنت تحتاج إلى توضيح أكثر أو تريد مزيدًا من الأمثلة، فلا تتردد في سؤالي.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "import faiss\n",
    "import numpy as np\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict\n",
    "import pickle\n",
    "import os\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Ensure GPU is used if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type != \"cuda\":\n",
    "    print(\"Warning: CUDA not available. Using CPU, which may be slower.\")\n",
    "\n",
    "# Initialize the BGE-M3 model for embeddings\n",
    "embedding_model = BGEM3FlagModel('BAAI/bge-m3', device=device)\n",
    "\n",
    "# Load the FAISS index\n",
    "cpu_index = faiss.read_index('subject_embeddings.faiss')\n",
    "\n",
    "# Move index to GPU if available\n",
    "if device.type == \"cuda\":\n",
    "    res = faiss.StandardGpuResources()\n",
    "    gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "else:\n",
    "    gpu_index = cpu_index  # Fallback to CPU index\n",
    "\n",
    "# Load metadata\n",
    "metadata_path = 'subject_metadata.pkl'\n",
    "with open(metadata_path, \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# Initialize the Qwen2.5-3B-Instruct model and tokenizer with Unsloth\n",
    "llm_model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "llm_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=llm_model_name,\n",
    "    max_seq_length=2048,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Initialize LangChain memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"query\",\n",
    "    output_key=\"content\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Define the prompt template with memory\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"query\", \"context\"],\n",
    "    template=\"\"\"انت معلم ذكي وودود، وظيفتك مساعدة الطلاب من جميع المراحل الدراسية العمرية عن طريق الرد على أسئلتهم في مواضيع الرياضيات، الأحياء، الكيمياء، الفيزياء، البيئة، اللغة العربية، والعلوم بطريقة بسيطة وسهلة وواضحة تتناسب مع مستواهم العمري وفقًا للسياق المقدم. يجب استخدام اللغة العربية فقط في الإجابة على أسئلة الطلاب، مع مراعاة تقديم الإجابات بشكل دقيق ومناسب للسياق.\n",
    "\n",
    "# تعليمات إضافية\n",
    "* تأكد من أن الإجابة خالية من التعقيدات ومباشرة.\n",
    "* إذا كان السؤال يتطلب شرحًا علميًا، استخدم لغة مبسطة تناسب المرحلة العمرية للطالب.\n",
    "* لا تستخدم مصطلحات معقدة إلا إذا كانت ضرورية، وفي هذه الحالة اشرحها ببساطة.\n",
    "* إذا كان هناك سجل محادثة سابق، استخدمه لضمان استمرارية السياق.\n",
    "\n",
    "سجل المحادثة السابقة:\n",
    "{chat_history}\n",
    "\n",
    "السؤال: \n",
    "{query}\n",
    "\n",
    "السياق:\n",
    "{context}\"\"\"\n",
    ")\n",
    "\n",
    "# Function to query FAISS without subject filter and no oversampling\n",
    "def search_faiss(query_text: str, index, embedding_model, metadata: List[Dict], top_k: int = 3) -> List[Dict]:\n",
    "    query_embedding = embedding_model.encode(query_text, max_length=256)['dense_vecs']\n",
    "    query_embedding = np.array([query_embedding]).astype('float32')\n",
    "\n",
    "    search_k = top_k\n",
    "    distances, indices = index.search(query_embedding, search_k)\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if idx >= len(metadata):\n",
    "            continue\n",
    "        result = {\n",
    "            'index': int(idx),\n",
    "            'distance': float(distances[0][i]),\n",
    "            'subject': metadata[idx]['subject'],\n",
    "            'text': metadata[idx]['text']\n",
    "        }\n",
    "        results.append(result)\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to call the Qwen25-3B-Instruct model with chat template\n",
    "def call_llm(prompt: str, max_new_tokens: int = 512) -> str:\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(llm_model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generated_ids = llm_model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "    # Decode the response\n",
    "    content = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "    return content\n",
    "\n",
    "# Conversational RAG function with memory\n",
    "def conversational_rag(query: str) -> Dict[str, str]:\n",
    "    # Search FAISS for relevant context\n",
    "    results = search_faiss(query, gpu_index, embedding_model, metadata, top_k=3)\n",
    "    context = \"\\n\".join([f\"- {result['text']} (موضوع: {result['subject']})\" for result in results])\n",
    "\n",
    "    # Get chat history from memory\n",
    "    chat_history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "    \n",
    "    # Format the prompt with history, query, and context\n",
    "    prompt = prompt_template.format(\n",
    "        chat_history=chat_history,\n",
    "        query=query,\n",
    "        context=context\n",
    "    )\n",
    "\n",
    "    # Generate response\n",
    "    response = call_llm(prompt)\n",
    "\n",
    "    # Save the query and response to memory\n",
    "    memory.save_context({\"query\": query}, {\"content\": response})\n",
    "\n",
    "    return {\"content\": response}\n",
    "\n",
    "# Example conversational loop\n",
    "def main():\n",
    "    print(\"مرحبًا يا أصدقاء! أنا معلم ودود هنا لمساعدتكم في الدراسة من الصف الأول إلى الصف الثاني عشر. اسألوني سؤالًا بالعربية (اكتب 'خروج' للخروج):\")\n",
    "    while True:\n",
    "        query = input(\"السؤال: \")\n",
    "        if query.strip() == \"خروج\":\n",
    "            break\n",
    "        result = conversational_rag(query)\n",
    "        print(f\"الإجابة: {result['content']}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88f70c-cc54-4191-98a0-d294b23f487a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
